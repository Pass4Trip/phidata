from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import json
import re
import traceback

import os
import openai
from phi.agent import Agent
from phi.model.openai import OpenAIChat
from phi.tools.python import PythonTools
from phi.tools.duckduckgo import DuckDuckGo

from agents.settings import agent_settings
from agents.web import get_web_searcher
from agents.api_knowledge import get_api_knowledge_agent
from agents.data_analysis import get_data_analysis_agent
from agents.travel_planner import get_travel_planner
from agents.orchestrator_prompts import (
    get_task_decomposition_prompt,
    get_task_execution_prompt,
    get_task_synthesis_prompt
)

from utils.colored_logging import get_colored_logger

import asyncio
from .agent_registry import agent_registry, AgentMetadata

logger = get_colored_logger('agents.orchestrator', 'OrchestratorAgent')

AGENT_ROUTING_PROMPT = """
Pour la t√¢che : '{task}'
Et les agents suivants :
{agents_description}

Historique des performances :
{performance_history}

Quel agent est le plus appropri√© pour cette t√¢che ?
Fournir le nom de l'agent, le score de confiance et le raisonnement.
Format : JSON avec les cl√©s 'selected_agent', 'confidence_score' et 'reasoning'
"""

TASK_CONTEXT_PROMPT = """
Pour la t√¢che : '{task}'
Et le contexte suivant :
{context}

Analyser le contexte et fournir les √©l√©ments suivants :
- Une analyse du contexte
- Une approche recommand√©e
- Des consid√©rations critiques
Format : JSON avec les cl√©s 'context_analysis', 'recommended_approach' et 'critical_considerations'
"""

@dataclass
class TaskLedger:
    """
    Registre pour g√©rer les faits et le plan de t√¢ches
    """
    original_request: str
    initial_plan: List[str] = field(default_factory=list)
    _current_plan: List[str] = field(default_factory=list, repr=False)
    facts: Dict[str, Any] = field(default_factory=lambda: {
        "verified": [],  # Faits v√©rifi√©s
        "to_lookup": [], # Faits √† rechercher
        "derived": [],   # Faits d√©riv√©s (calculs/logique)
        "guesses": []    # Suppositions √©duqu√©es
    })
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    context: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not isinstance(self._current_plan, list):
            if isinstance(self._current_plan, dict):
                self._current_plan = list(self._current_plan.values())
            elif isinstance(self._current_plan, str):
                self._current_plan = [self._current_plan]
            else:
                self._current_plan = [str(self._current_plan)] if self._current_plan is not None else []

    @property
    def current_plan(self) -> List[str]:
        return self._current_plan

    @current_plan.setter
    def current_plan(self, value: Union[List[str], Dict[str, Any], str, Any]):
        if not isinstance(value, list):
            if isinstance(value, dict):
                value = list(value.values())
            elif isinstance(value, str):
                value = [value]
            else:
                value = [str(value)] if value is not None else []
        self._current_plan = value
        self.updated_at = datetime.now()

    def add_fact(self, fact: str, fact_type: str = "verified"):
        """Ajouter un fait au registre"""
        if fact_type in self.facts:
            if fact not in self.facts[fact_type]:
                self.facts[fact_type].append(fact)
                self.updated_at = datetime.now()

    def add_task(self, task: str):
        """Ajouter une t√¢che au plan courant"""
        if task not in self._current_plan:
            self._current_plan.append(task)
            self.updated_at = datetime.now()

    def register_agent(self, agent: Agent, agent_name: str):
        """Enregistrer un agent dans le registre"""
        self.context[agent_name] = agent

    def to_json(self) -> Dict[str, Any]:
        """
        Convertir le TaskLedger en format JSON
        """
        return {
            "original_request": self.original_request,
            "initial_plan": self.initial_plan,
            "current_plan": self.current_plan,
            "facts": self.facts,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "context": self.context
        }

@dataclass
class ProgressLedger:
    """
    Registre pour suivre la progression et g√©rer les blocages
    """
    task_ledger: TaskLedger
    completed_tasks: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    task_history: List[Dict[str, Any]] = field(default_factory=list)
    agent_performance: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    stall_count: int = 0
    max_stalls: int = 2
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    status: str = "pending"

    def is_task_complete(self) -> bool:
        """V√©rifie si toutes les t√¢ches sont termin√©es"""
        return len(self.task_ledger.current_plan) == 0

    def is_making_progress(self) -> bool:
        """V√©rifie si des progr√®s sont r√©alis√©s"""
        if not self.task_history:
            return True
        last_update = datetime.fromisoformat(self.task_history[-1]["completed_at"])
        time_since_last_update = datetime.now() - last_update
        return time_since_last_update.seconds < 300  # 5 minutes

    def is_stalled(self) -> bool:
        """V√©rifie si l'ex√©cution est bloqu√©e"""
        return self.stall_count >= self.max_stalls

    def complete_task(self, task: str, result: Dict[str, Any]):
        """Marquer une t√¢che comme termin√©e"""
        agent_name = result.get('agent', 'unknown')
        
        # Mettre √† jour les performances de l'agent
        if agent_name not in self.agent_performance:
            self.agent_performance[agent_name] = {
                'tasks_completed': 0,
                'success_rate': 1.0,
                'total_tasks': 0,
                'recent_performance': []
            }
        
        performance = self.agent_performance[agent_name]
        performance['total_tasks'] += 1
        
        if 'error' not in result:
            performance['tasks_completed'] += 1
            performance['success_rate'] = performance['tasks_completed'] / performance['total_tasks']
            performance['recent_performance'].append('success')
        else:
            performance['recent_performance'].append('failure')
        
        # Stocker le r√©sultat
        self.completed_tasks[task] = result
        self.task_history.append({
            "task": task,
            "result": result,
            "completed_at": datetime.now().isoformat()
        })
        
        # Retirer la t√¢che du plan courant
        if task in self.task_ledger.current_plan:
            self.task_ledger.current_plan.remove(task)
        
        self.updated_at = datetime.now()

    def increment_stall(self):
        """Incr√©menter le compteur de blocages"""
        self.stall_count += 1
        self.updated_at = datetime.now()

    def reset_stall(self):
        """R√©initialiser le compteur de blocages"""
        self.stall_count = 0
        self.updated_at = datetime.now()

    def get_next_agent(self, task: str) -> str:
        """
        S√©lectionner le prochain agent bas√© sur les performances
        """
        best_agent = None
        best_score = -1

        for agent_name, perf in self.agent_performance.items():
            score = perf['success_rate'] * (perf['tasks_completed'] + 1)
            if score > best_score:
                best_score = score
                best_agent = agent_name

        return best_agent or "API Knowledge Agent"  # Agent par d√©faut

    def to_json(self) -> Dict[str, Any]:
        """Convertir en format JSON"""
        def convert_result(result):
            if hasattr(result, 'content'):
                return {
                    "content": result.content,
                    "content_type": str(result.content_type),
                    "event": str(result.event)
                }
            return result

        return {
            "task_ledger": self.task_ledger.to_json(),
            "completed_tasks": {
                task: {k: convert_result(v) for k, v in result.items()} 
                for task, result in self.completed_tasks.items()
            },
            "task_history": [
                {
                    "task": entry["task"],
                    "result": convert_result(entry["result"]),
                    "completed_at": entry["completed_at"]
                } 
                for entry in self.task_history
            ],
            "agent_performance": self.agent_performance,
            "stall_count": self.stall_count,
            "status": self.status
        }

class OrchestratorAgent:
    """
    Agent orchestrateur avanc√© avec d√©composition de t√¢ches
    """
    def __init__(
        self,
        model_id: str = "gpt-4o-mini", 
        debug_mode: bool = False,
        original_request: Optional[str] = None,
        api_key: Optional[str] = None  # Ajout du param√®tre api_key
    ):
        """
        Initialiser l'agent orchestrateur avec des agents sp√©cialis√©s
        
        Args:
            model_id (str): Identifiant du mod√®le OpenAI
            debug_mode (bool): Mode de d√©bogage
            original_request (Optional[str]): Requ√™te originale pour le TaskLedger
            api_key (Optional[str]): Cl√© API OpenAI personnalis√©e
        """
        # Configuration du mod√®le LLM
        self.llm_config = {
            "model": model_id,
            "temperature": 0.2
        }
        
        # Utiliser la cl√© API fournie ou celle de l'environnement
        openai_api_key = api_key or os.getenv('OPENAI_API_KEY')
        
        # Initialisation explicite du client OpenAI
        try:
            self.client = openai.OpenAI(api_key=openai_api_key)
        except Exception as e:
            logger.error(f"‚ùå Erreur d'initialisation du client OpenAI : {e}")
            self.client = None
        
        # Initialisation centralis√©e des agents
        self.agents = self._initialize_specialized_agents()
        
        # Initialisation du mode de d√©bogage
        self.debug_mode = debug_mode
        
        # Cr√©ation du TaskLedger initial
        self.task_ledger = self._create_task_ledger(original_request)
        
        # Cr√©er l'agent orchestrateur avec configuration simplifi√©e
        self.agent = self._create_orchestrator_agent(debug_mode)

    def _initialize_specialized_agents(self) -> Dict[str, Agent]:
        """
        Initialiser tous les agents sp√©cialis√©s de mani√®re centralis√©e
        
        Returns:
            Dict[str, Agent]: Dictionnaire des agents disponibles
        """
        return {
            "web_search": get_web_searcher(
                model_id=agent_settings.gpt_4,
                debug_mode=False,
                name="Web Search Agent"
            ),
            "api_knowledge": get_api_knowledge_agent(
                debug_mode=False,
                user_id=None,
                session_id=None
            ),
            "travel_planner": get_travel_planner(
                model_id=agent_settings.gpt_4,
                debug_mode=False,
                name="Travel Planner Agent"
            )
        }

    def _create_task_ledger(self, original_request: Optional[str] = None) -> TaskLedger:
        """
        Cr√©er un TaskLedger avec gestion robuste
        
        Args:
            original_request (Optional[str]): Requ√™te originale
        
        Returns:
            TaskLedger: Registre de t√¢ches initialis√©
        """
        return TaskLedger(
            original_request=original_request or "Requ√™te non sp√©cifi√©e",
            context={name: agent for name, agent in self.agents.items()}
        )

    def _create_orchestrator_agent(self, debug_mode: bool) -> Agent:
        """
        Cr√©er l'agent orchestrateur avec configuration unifi√©e
        
        Args:
            debug_mode (bool): Mode de d√©bogage
        
        Returns:
            Agent: Agent orchestrateur configur√©
        """
        return Agent(
            llm=OpenAIChat(**self.llm_config),
            tools=[
                PythonTools(),
                DuckDuckGo()
            ],
            instructions=[
                "Tu es un agent d'orchestration avanc√© capable de d√©composer des t√¢ches complexes.",
                "√âtapes de travail :",
                "1. Analyser la requ√™te initiale",
                "2. D√©composer en sous-t√¢ches pr√©cises",
                "3. S√©lectionner l'agent le plus appropri√©",
                "4. Coordonner l'ex√©cution des sous-t√¢ches",
                "5. Int√©grer et synth√©tiser les r√©sultats partiels",
                "6. Adapter dynamiquement le plan si n√©cessaire"
            ],
            team=list(self.agents.values()),
            debug_mode=debug_mode,
            name="Advanced Task Orchestrator"
        )

    def _get_task_decomposition_functions(self) -> List[Dict[str, Any]]:
        """
        D√©finir les fonctions d'appel pour la d√©composition de t√¢ches
        
        Returns:
            List[Dict[str, Any]]: Liste des d√©finitions de fonctions
        """
        return [
            {
                "name": "decompose_task",
                "description": "D√©composer une t√¢che complexe en sous-t√¢ches pr√©cises et ordonn√©es",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "subtasks": {
                            "type": "array",
                            "description": "Liste des sous-t√¢ches",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "task": {
                                        "type": "string", 
                                        "description": "Description pr√©cise de la sous-t√¢che"
                                    },
                                    "agent": {
                                        "type": "string", 
                                        "description": "Agent recommand√© pour la sous-t√¢che",
                                        "enum": [
                                            "Web Search Agent", 
                                            "API Knowledge Agent", 
                                            "Travel Planner Agent"
                                        ]
                                    },
                                    "priority": {
                                        "type": "string", 
                                        "description": "Priorit√© de la sous-t√¢che",
                                        "enum": ["haute", "moyenne", "basse"]
                                    }
                                },
                                "required": ["task", "agent", "priority"]
                            }
                        }
                    },
                    "required": ["subtasks"]
                }
            }
        ]

    def _get_agent_selection_functions(self) -> List[Dict[str, Any]]:
        """
        D√©finir les fonctions d'appel pour la s√©lection d'agents
        
        Returns:
            List[Dict[str, Any]]: Liste des d√©finitions de fonctions
        """
        return [
            {
                "name": "select_best_agent",
                "description": "S√©lectionner l'agent le plus appropri√© pour une t√¢che donn√©e",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "task": {
                            "type": "string", 
                            "description": "Description de la t√¢che √† ex√©cuter"
                        },
                        "selected_agent": {
                            "type": "object",
                            "description": "Agent s√©lectionn√© avec son score de confiance",
                            "properties": {
                                "name": {
                                    "type": "string", 
                                    "description": "Nom de l'agent",
                                    "enum": [
                                        "Web Search Agent", 
                                        "API Knowledge Agent", 
                                        "Travel Planner Agent"
                                    ]
                                },
                                "confidence_score": {
                                    "type": "number", 
                                    "description": "Score de confiance (0.0-1.0)",
                                    "minimum": 0,
                                    "maximum": 1
                                },
                                "reasoning": {
                                    "type": "string", 
                                    "description": "Justification de la s√©lection"
                                }
                            },
                            "required": ["name", "confidence_score", "reasoning"]
                        }
                    },
                    "required": ["task", "selected_agent"]
                }
            }
        ]

    def should_decompose_task(self, user_request: str) -> bool:
        """
        Utiliser le LLM pour d√©terminer si la t√¢che n√©cessite une d√©composition
        
        Args:
            user_request (str): La requ√™te utilisateur originale
        
        Returns:
            bool: True si d√©composition n√©cessaire, False sinon
        """
        try:
            response = self.client.chat.completions.create(
                model=self.llm_config["model"],
                messages=[
                    {
                        "role": "system", 
                        "content": """
                        Tu es un expert en analyse de t√¢ches complexes.
                        
                        CRIT√àRES DE D√âCOMPOSITION :
                        - La t√¢che n√©cessite-t-elle vraiment d'√™tre divis√©e ?
                        - Pr√©sente-t-elle plusieurs dimensions ou √©tapes ?
                        - Requiert-elle diff√©rentes comp√©tences ou approches ?
                        
                        NE PAS D√âCOMPOSER pour :
                        - Requ√™tes simples et directes
                        - T√¢ches ne n√©cessitant qu'une seule action
                        - Demandes courtes et pr√©cises
                        
                        D√âCOMPOSER pour :
                        - Projets multi-√©tapes
                        - T√¢ches n√©cessitant planification
                        - Requ√™tes complexes avec plusieurs objectifs
                        
                        R√©ponds par "OUI" ou "NON" avec discernement
                        """
                    },
                    {
                        "role": "user", 
                        "content": f"Analyse cette requ√™te : {user_request}"
                    }
                ],
                max_tokens=10,
                temperature=0.2
            )
            
            llm_decision = response.choices[0].message.content.strip().upper()
            logger.info(f"D√©cision de d√©composition pour '{user_request}': {llm_decision}")
            
            return llm_decision == "OUI"
        
        except Exception as e:
            logger.error(f"Erreur lors de la d√©cision de d√©composition : {e}")
            return False

    def _select_best_agent(self, task: str) -> Agent:
        """
        S√©lectionner l'agent le plus appropri√© avec function calling
        
        Args:
            task (str): T√¢che √† ex√©cuter
        
        Returns:
            Agent: Agent s√©lectionn√©
        """
        try:
            # V√©rifier si le client est initialis√©
            if self.client is None:
                logger.warning("Client OpenAI non initialis√©. Utilisation de l'agent par d√©faut.")
                return next(iter(self.agents.values()))

            # Utiliser le function calling pour la s√©lection d'agent
            messages = [
                {"role": "system", "content": "Tu es un agent d'orchestration capable de s√©lectionner l'agent le plus appropri√© pour une t√¢che."},
                {"role": "user", "content": f"S√©lectionne l'agent le plus appropri√© pour la t√¢che : {task}"}
            ]
            
            response = self.client.chat.completions.create(
                model=self.llm_config.get('model', 'gpt-4o-mini'),
                messages=messages,
                functions=self._get_agent_selection_functions(),
                function_call={"name": "select_best_agent"}
            )
            
            # Extraire l'agent s√©lectionn√©
            function_call = response.choices[0].message.function_call
            selection_data = json.loads(function_call.arguments)
            selected_agent_name = selection_data.get('selected_agent', {}).get('name', 'Web Search Agent')
            
            # Convertir le nom en agent
            selected_agent = self.agents.get(selected_agent_name.lower().replace(' ', '_'), 
                                             next(iter(self.agents.values())))
            
            return selected_agent
        
        except Exception as e:
            logger.warning(f"Erreur de s√©lection d'agent : {e}. Utilisation de l'agent par d√©faut.")
            return next(iter(self.agents.values()))

    def _parse_json_response(self, response: Any) -> List[Dict[str, str]]:
        """
        Parser une r√©ponse JSON de mani√®re robuste
        
        Args:
            response (Any): La r√©ponse √† parser
        
        Returns:
            List[Dict[str, str]]: Liste des sous-t√¢ches pars√©es
        """
        try:
            # Extraction du function call
            function_call = response.choices[0].message.function_call
            subtasks_data = json.loads(function_call.arguments)
            subtasks = subtasks_data.get('subtasks', [])
            
            # V√©rification et enrichissement des sous-t√¢ches
            if not subtasks:
                # G√©n√©ration de sous-t√¢ches d√©taill√©es si vide
                subtasks = self._generate_detailed_subtasks(self.task_ledger.original_request)
            
            # Validation et compl√©tion des sous-t√¢ches
            validated_subtasks = []
            for subtask in subtasks:
                validated_task = {
                    'task': subtask.get('task', 'T√¢che non sp√©cifi√©e'),
                    'agent': subtask.get('agent', 'Web Search Agent'),
                    'priority': subtask.get('priority', 'moyenne')
                }
                validated_subtasks.append(validated_task)
            
            return validated_subtasks
        
        except Exception as e:
            logger.warning(f"Erreur de parsing JSON : {e}")
            # G√©n√©ration de sous-t√¢ches par d√©faut
            return self._generate_detailed_subtasks(self.task_ledger.original_request)

    def _generate_detailed_subtasks(self, request: str) -> List[Dict[str, str]]:
        """
        G√©n√©rer des sous-t√¢ches d√©taill√©es bas√©es sur la requ√™te
        """
        try:
            # Premi√®re √©tape : identification de l'objectif global
            global_objective_response = self.client.chat.completions.create(
                model=self.llm_config["model"],
                messages=[
                    {
                        "role": "system",
                        "content": """
                        Tu es un expert en analyse strat√©gique et d√©finition d'objectifs.
                        
                        INSTRUCTIONS CRUCIALES :
                        1. Analyse la requ√™te de mani√®re NEUTRE et OBJECTIVE
                        
                        2. Si la requ√™te contient des r√©f√©rences temporelles relatives (ex: 'demain', 'dans une semaine', 'hier'):
                           - Utilise la date actuelle ({current_date}) comme r√©f√©rence
                           - Convertis ces r√©f√©rences en dates pr√©cises avant d'effectuer la recherche
                           - Exemple: '√©v√©nements de demain' -> '√©v√©nements du {tomorrow_date}'
                        
                        3. Extraire l'OBJECTIF GLOBAL d'une requ√™te
                           - √ätre concis et pr√©cis
                           - Capturer l'essence de la demande
                        
                        FORMAT :
                        {
                            "global_objective": "Description claire et concise",
                            "objective_type": "r√©solution/exploration/planification/...",
                            "key_dimensions": ["dimension1", "dimension2"],
                            "initial_constraints": ["contrainte1", "contrainte2"]
                        }
                        """.format(
                            current_date=datetime.now().strftime("%Y-%m-%d"),
                            tomorrow_date=(datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
                        )
                    },
                    {
                        "role": "user",
                        "content": request
                    }
                ],
                response_format={"type": "json_object"},
                max_tokens=300,
                temperature=0.3
            )
            
            global_objective_data = json.loads(global_objective_response.choices[0].message.content)
            
            # Deuxi√®me √©tape : g√©n√©ration des sous-t√¢ches avec r√©sultats cumulatifs
            response = self.client.chat.completions.create(
                model=self.llm_config["model"],
                messages=[
                    {
                        "role": "system", 
                        "content": """
                        Tu es un expert en d√©composition de t√¢ches s√©quentielles avec une strat√©gie de r√©duction de p√©rim√®tre.

                        PRINCIPES FONDAMENTAUX :
                        1. ORDONNANCEMENT STRATEGIQUE des sous-t√¢ches
                           - Chaque sous-t√¢che R√âDUIT PROGRESSIVEMENT le champ de recherche
                           - Minimiser le p√©rim√®tre de la t√¢che suivante
                           - Cr√©er une LOGIQUE D√âICTIQUE (qui se resserre)

                        2. CRIT√àRES D'ORDONNANCEMENT :
                           - Commencer par les t√¢ches qui √âLARGISSENT le champ
                           - Progresser vers des t√¢ches de plus en plus SP√âCIFIQUES
                           - Chaque √©tape LIMITE le p√©rim√®tre de la suivante

                        3. STRAT√âGIE DE R√âDUCTION :
                           - T√¢che 1 : Vue large, exploration g√©n√©rale
                           - T√¢che 2 : Filtrage, r√©duction du champ
                           - T√¢che N : Pr√©cision maximale, r√©sultat final

                        4. CONTRAINTES D'EX√âCUTION :
                           - Chaque sous-t√¢che UTILISE les r√©sultats de la pr√©c√©dente
                           - R√©duire EXPONENTIELLEMENT le p√©rim√®tre de recherche
                           - Garantir une progression logique et efficace

                        FORMAT IMP√âRATIF :
                        [
                            {
                                "task": "Description pr√©cise",
                                "agent": "Agent optimal",
                                "priority": "haute/moyenne/basse",
                                "contribution": "R√¥le dans l'objectif global",
                                "input_constraints": ["r√©sultat t√¢che pr√©c√©dente"],
                                "output_constraints": ["r√©sultat √† produire"],
                                "reduction_ratio": "Pourcentage de r√©duction du p√©rim√®tre"
                            }
                        ]
                        """
                    },
                    {
                        "role": "user", 
                        "content": f"""
                        Requ√™te originale : {request}
                        
                        OBJECTIF GLOBAL :
                        - Description : {global_objective_data.get('global_objective', 'Non d√©fini')}
                        - Type : {global_objective_data.get('objective_type', 'Non sp√©cifi√©')}
                        - Dimensions cl√©s : {', '.join(global_objective_data.get('key_dimensions', []))}
                        - Contraintes initiales : {', '.join(global_objective_data.get('initial_constraints', []))}
                        
                        CONSIGNE CRUCIALE : 
                        - D√©compose en sous-t√¢ches s√©quentielles
                        - CHAQUE sous-t√¢che UTILISE les r√©sultats des t√¢ches pr√©c√©dentes
                        - R√©duire PROGRESSIVEMENT le p√©rim√®tre de recherche
                        - Chaque r√©sultat LIMITE le champ de la t√¢che suivante
                        """
                    }
                ],
                response_format={"type": "json_object"},
                max_tokens=500,
                temperature=0.4
            )
            
            result = json.loads(response.choices[0].message.content)
            subtasks = result.get('subtasks', [])
            
            return subtasks
        
        except Exception as e:
            logger.error(f"Erreur de g√©n√©ration de sous-t√¢ches : {e}")
            return [{
                'task': "Analyse strat√©gique globale",
                'agent': 'Multi-Purpose Agent',
                'priority': 'haute',
                'contribution': f"D√©composition et r√©solution de la requ√™te complexe : {request}",
                'input_constraints': [],
                'output_constraints': ["Fournir une analyse compl√®te"]
            }]

    def _get_dict_value(self, obj: Any, key: str, default: Any = None) -> Any:
        """
        R√©cup√©rer une valeur d'un dictionnaire de mani√®re s√©curis√©e
        """
        try:
            if hasattr(obj, 'dict'):
                # Pour les objets Pydantic
                return obj.dict().get(key, default)
            elif hasattr(obj, 'get'):
                # Pour les dictionnaires
                return obj.get(key, default)
            elif hasattr(obj, key):
                # Pour les objets avec attributs
                return getattr(obj, key)
            else:
                return default
        except Exception:
            return default

    def _extract_content(self, result: Any) -> str:
        """
        Extraire le contenu d'un r√©sultat de diff√©rents types avec une gestion robuste
        
        Args:
            result (Any): Le r√©sultat √† extraire
        
        Returns:
            str: Le contenu extrait
        """
        # Gestion explicite des RunResponse
        if hasattr(result, 'content'):
            # V√©rifier le type de contenu
            if isinstance(result.content, str):
                return result.content
            elif isinstance(result.content, dict):
                return json.dumps(result.content)
            elif result.content is None:
                return ""
            else:
                return str(result.content)
        
        # Autres types de r√©sultats
        if result is None:
            return ""
        
        # Gestion des dictionnaires et listes
        if isinstance(result, (dict, list)):
            return json.dumps(result)
        
        # Conversion en cha√Æne par d√©faut
        return str(result)

    async def decompose_task(self, user_request: str) -> TaskLedger:
        """
        D√©composer la requ√™te utilisateur en sous-t√¢ches avec function calling
        
        Args:
            user_request (str): La requ√™te utilisateur originale
        
        Returns:
            TaskLedger: Le registre de t√¢ches mis √† jour
        """
        # 1. V√©rification pr√©liminaire de d√©composition
        needs_decomposition = self.should_decompose_task(user_request)
        
        # Ajouter un fait sur la d√©cision de d√©composition
        self.task_ledger.add_fact(
            f"D√©composition n√©cessaire : {needs_decomposition}", 
            fact_type="derived"
        )
        
        # 2. Si pas besoin de d√©composition, traitement simple
        if not needs_decomposition:
            # Ajouter directement la t√¢che au registre
            self.task_ledger.add_task(user_request)
            logger.info(f"""
Task Ledger g√©n√©r√© :
    - Requ√™te utilisateur : {user_request}
    - Nombre de sous-t√¢ches : 1
""")
            return self.task_ledger
        
        # 3. Logique existante de d√©composition avec function calling
        try:
            # Appel au LLM pour d√©composer la t√¢che
            response = self.client.chat.completions.create(
                model=self.llm_config["model"],
                messages=[
                    {"role": "system", "content": "D√©compose la t√¢che en sous-t√¢ches pr√©cises et ordonn√©es"},
                    {"role": "user", "content": user_request}
                ],
                functions=self._get_task_decomposition_functions(),
                function_call={"name": "decompose_task"}
            )
            
            # 4. Parsing de la r√©ponse
            decomposition = self._parse_json_response(response)
            
            # 5. Ajouter les sous-t√¢ches au registre
            for subtask in decomposition:
                self.task_ledger.add_task(subtask['task'])
                
                # Ajouter un fait sur chaque sous-t√¢che
                self.task_ledger.add_fact(
                    f"Sous-t√¢che identifi√©e : {subtask['task']}", 
                    fact_type="derived"
                )
            
            logger.info(f"""
                Task Ledger g√©n√©r√© :
                - Requ√™te utilisateur : {user_request}
                - Nombre de sous-t√¢ches : {len(decomposition)}
                - D√©tail des sous-t√¢ches :
                {json.dumps(decomposition, indent=2)}
                """)
            
            return self.task_ledger
        
        except Exception as e:
            # Gestion des erreurs
            logger.error(f"Erreur lors de la d√©composition : {e}")
            
            # Repli : ajouter la t√¢che originale
            self.task_ledger.add_task(user_request)
            self.task_ledger.add_fact(
                f"√âchec de d√©composition, t√¢che originale conserv√©e", 
                fact_type="guesses"
            )
        
        return self.task_ledger

    async def execute_task(self, task_ledger: TaskLedger) -> Dict[str, Any]:
        """
        Ex√©cuter les sous-t√¢ches de mani√®re unifi√©e
        
        Args:
            task_ledger (TaskLedger): Le registre de t√¢ches √† ex√©cuter
        
        Returns:
            Dict[str, Any]: R√©sultats de l'ex√©cution des t√¢ches
        """
        task_results = {}
        
        try:
            logger.info("üìã D√©but de l'ex√©cution des sous-t√¢ches")
            logger.info(f"üî¢ Nombre total de sous-t√¢ches : {len(task_ledger.current_plan)}")
            
            for task_index, current_task in enumerate(task_ledger.current_plan[:], 1):
                try:
                    # S√©lection dynamique de l'agent
                    selected_agent = self._select_best_agent(current_task)
                    
                    # Ex√©cution de la t√¢che
                    logger.info(f"üöÄ Ex√©cution de la sous-t√¢che {task_index}/{len(task_ledger.current_plan)}")
                    logger.info(f"üìù T√¢che : {current_task}")
                    logger.info(f"ü§ñ Agent s√©lectionn√© : {selected_agent.name}")
                    
                    result = await selected_agent.arun(current_task)
                    
                    # Extraction du contenu
                    result_content = self._extract_content(result)
                    
                    # Stocker le r√©sultat
                    task_results[current_task] = {
                        "agent": selected_agent.name,
                        "result": result_content
                    }
                    
                    logger.info(f"‚úÖ Sous-t√¢che {task_index} termin√©e")
                    logger.info(f"üìä R√©sultat : {result_content[:200]}...")
                
                except Exception as task_error:
                    logger.error(f"‚ùå Erreur lors de l'ex√©cution de la sous-t√¢che {task_index} : {task_error}")
                    logger.error(traceback.format_exc())
                    
                    task_results[current_task] = {
                        "error": str(task_error),
                        "traceback": traceback.format_exc()
                    }
            
            logger.info("üèÅ Ex√©cution de toutes les sous-t√¢ches termin√©e")
            
            return task_results
        
        except Exception as global_error:
            logger.error(f"‚ùå Erreur globale lors de l'ex√©cution des t√¢ches : {global_error}")
            logger.error(traceback.format_exc())
            
            return {
                "error": str(global_error),
                "task_results": task_results
            }

    async def process_request(
        self, 
        user_request: str, 
        debug_mode: bool = False
    ) -> Dict[str, Any]:
        """
        Traiter une requ√™te de bout en bout
        
        Args:
            user_request (str): La requ√™te utilisateur
            debug_mode (bool): Mode de d√©bogage
        
        Returns:
            Dict[str, Any]: R√©sultats du traitement
        """
        try:
            # D√©composition de la t√¢che
            task_ledger = await self.decompose_task(user_request)
            
            # Ex√©cution des sous-t√¢ches
            task_results = await self.execute_task(task_ledger)
            
            # Synth√®se des r√©sultats
            synthesis_result = await self._synthesize_results(task_results)
            
            return {
                "task_ledger": task_ledger.to_json(),
                "task_results": task_results,
                "final_result": synthesis_result
            }
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement de la requ√™te : {e}")
            logger.error(traceback.format_exc())
            
            return {
                "error": str(e),
                "traceback": traceback.format_exc()
            }

    async def _synthesize_results(self, task_results: Dict[str, Any]) -> str:
        """
        Synth√©tiser les r√©sultats des sous-t√¢ches
        
        Args:
            task_results (Dict[str, Any]): R√©sultats des sous-t√¢ches
        
        Returns:
            str: Synth√®se des r√©sultats
        """
        try:
            synthesis_prompt = TASK_CONTEXT_PROMPT.format(
                task=self.task_ledger.original_request,
                context=json.dumps(task_results)
            )
            
            return self.agent.run(synthesis_prompt)
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la synth√®se des r√©sultats : {e}")
            return "Impossible de synth√©tiser les r√©sultats."

async def process_user_request(
    user_request: str, 
    debug_mode: bool = False
) -> Dict[str, Any]:
    """
    Traiter une requ√™te utilisateur de mani√®re asynchrone avec l'orchestrateur
    
    Args:
        user_request (str): La requ√™te de l'utilisateur
        debug_mode (bool): Mode de d√©bogage
    
    Returns:
        Dict[str, Any]: R√©sultat du traitement de la requ√™te
    """
    try:
        orchestrator = OrchestratorAgent(
            debug_mode=debug_mode, 
            original_request=user_request
        )
        result = await orchestrator.process_request(
            user_request=user_request,
            debug_mode=debug_mode
        )
        return result
    except Exception as e:
        logger.error(f"Erreur lors du traitement de la requ√™te : {e}")
        logger.error(traceback.format_exc())
        return {
            "error": f"Erreur lors du traitement de la requ√™te : {e}",
            "query": user_request,
            "result": None
        }

def get_orchestrator_agent(
    model_id: str = "gpt-4o-mini"
) -> Agent:
    """
    Cr√©er et configurer l'agent orchestrateur principal
    
    Args:
        model_id (str): Identifiant du mod√®le OpenAI √† utiliser
    
    Returns:
        Agent: Agent orchestrateur configur√©
    """
    web_agent = get_web_searcher(
        model_id=agent_settings.gpt_4,
        debug_mode=False
    )
    api_knowledge_agent = get_api_knowledge_agent(
        debug_mode=False,
        user_id=None,
        session_id=None
    )
    data_analysis_agent = get_data_analysis_agent(
        debug_mode=False
    )
    travel_planner = get_travel_planner(
        debug_mode=False
    )

    return Agent(
        name="Advanced Task Orchestrator",
        role="D√©composer et coordonner des t√¢ches complexes de mani√®re autonome",
        model=OpenAIChat(
            id=model_id,
            max_tokens=agent_settings.default_max_completion_tokens,
            temperature=0.7  # Plus cr√©atif pour la d√©composition
        ),
        instructions=[
            "Tu es un agent d'orchestration avanc√© capable de d√©composer des t√¢ches complexes.",
            "√âtapes de travail :",
            "1. Analyser la requ√™te originale",
            "2. D√©composer la requ√™te en sous-t√¢ches pr√©cises et r√©alisables",
            "3. Attribuer chaque sous-t√¢che √† l'agent le plus appropri√©",
            "4. Suivre la progression de chaque sous-t√¢che",
            "5. Int√©grer et synth√©tiser les r√©sultats partiels",
            "6. Adapter dynamiquement le plan si n√©cessaire"
        ],
        tools=[
            PythonTools(),
            DuckDuckGo()
        ],
        team=[
            web_agent,
            api_knowledge_agent,
            data_analysis_agent,
            travel_planner
        ]
    )

# Exemple d'utilisation
if __name__ == "__main__":
    import asyncio
    async def main():
        test_request = "Trouve des informations sur l'intelligence artificielle et r√©sume-les"
        result = await process_user_request(test_request, debug_mode=True)
        print(result)
    asyncio.run(main())