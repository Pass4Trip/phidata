import os
import sys
import asyncio
import logging
import json
import uuid
import traceback
from typing import Any, Dict, List, Optional, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from phi.agent import RunResponse, Agent
import openai

from agents.web import get_web_searcher
from agents.settings import agent_settings
from agents.orchestrator_prompts import (
    get_task_decomposition_prompt,
    get_task_execution_prompt,
    get_task_synthesis_prompt
)

from utils.colored_logging import get_colored_logger

# Ajouter le r√©pertoire parent au PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

# Configuration du logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# Ajout d'un handler de console si n√©cessaire
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)

# Ajouter le handler au logger s'il n'est pas d√©j√† pr√©sent
if not logger.handlers:
    logger.addHandler(console_handler)

AGENT_ROUTING_PROMPT = """
Pour la t√¢che : '{task}'
Et les agents suivants :
{agents_description}

Historique des performances :
{performance_history}

Quel agent est le plus appropri√© pour cette t√¢che ?
Fournir le nom de l'agent, le score de confiance et le raisonnement.
Format : JSON avec les cl√©s 'selected_agent', 'confidence_score' et 'reasoning'
"""

TASK_CONTEXT_PROMPT = """
Pour la t√¢che : '{task}'
Et le contexte suivant :
{context}

Analyser le contexte et fournir les √©l√©ments suivants :
- Une analyse du contexte
- Une approche recommand√©e
- Des consid√©rations critiques
Format : JSON avec les cl√©s 'context_analysis', 'recommended_approach' et 'critical_considerations'
"""

@dataclass
class TaskLedger:
    """
    Registre pour g√©rer les faits et le plan de t√¢ches
    """
    original_request: str
    initial_plan: List[str] = field(default_factory=list)
    _current_plan: List[str] = field(default_factory=list, repr=False)
    facts: Dict[str, Any] = field(default_factory=lambda: {
        "verified": [],  # Faits v√©rifi√©s
        "to_lookup": [], # Faits √† rechercher
        "derived": [],   # Faits d√©riv√©s (calculs/logique)
        "guesses": []    # Suppositions √©duqu√©es
    })
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    context: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not isinstance(self._current_plan, list):
            if isinstance(self._current_plan, dict):
                self._current_plan = list(self._current_plan.values())
            elif isinstance(self._current_plan, str):
                self._current_plan = [self._current_plan]
            else:
                self._current_plan = [str(self._current_plan)] if self._current_plan is not None else []

    @property
    def current_plan(self) -> List[str]:
        return self._current_plan

    @current_plan.setter
    def current_plan(self, value: Union[List[str], Dict[str, Any], str, Any]):
        if not isinstance(value, list):
            if isinstance(value, dict):
                value = list(value.values())
            elif isinstance(value, str):
                value = [value]
            else:
                value = [str(value)] if value is not None else []
        self._current_plan = value
        self.updated_at = datetime.now()

    def add_fact(self, fact: str, fact_type: str = "verified"):
        """Ajouter un fait au registre"""
        if fact_type in self.facts:
            if fact not in self.facts[fact_type]:
                self.facts[fact_type].append(fact)
                self.updated_at = datetime.now()

    def add_task(self, task: str):
        """Ajouter une t√¢che au plan courant"""
        if task not in self._current_plan:
            self._current_plan.append(task)
            self.updated_at = datetime.now()

    def register_agent(self, agent: Agent, agent_name: str):
        """Enregistrer un agent dans le registre"""
        self.context[agent_name] = agent

    def to_json(self) -> Dict[str, Any]:
        """
        Convertir le TaskLedger en format JSON
        """
        return {
            "original_request": self.original_request,
            "initial_plan": self.initial_plan,
            "current_plan": self.current_plan,
            "facts": self.facts,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "context": self.context
        }

@dataclass
class ProgressLedger:
    """
    Registre pour suivre la progression et g√©rer les blocages
    """
    task_ledger: TaskLedger
    completed_tasks: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    task_history: List[Dict[str, Any]] = field(default_factory=list)
    agent_performance: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    stall_count: int = 0
    max_stalls: int = 2
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    status: str = "pending"

    def is_task_complete(self) -> bool:
        """V√©rifie si toutes les t√¢ches sont termin√©es"""
        return len(self.task_ledger.current_plan) == 0

    def is_making_progress(self) -> bool:
        """V√©rifie si des progr√®s sont r√©alis√©s"""
        if not self.task_history:
            return True
        last_update = datetime.fromisoformat(self.task_history[-1]["completed_at"])
        time_since_last_update = datetime.now() - last_update
        return time_since_last_update.seconds < 300  # 5 minutes

    def is_stalled(self) -> bool:
        """V√©rifie si l'ex√©cution est bloqu√©e"""
        return self.stall_count >= self.max_stalls

    def complete_task(self, task: str, result: Dict[str, Any]):
        """Marquer une t√¢che comme termin√©e"""
        agent_name = result.get('agent', 'unknown')
        
        # Mettre √† jour les performances de l'agent
        if agent_name not in self.agent_performance:
            self.agent_performance[agent_name] = {
                'tasks_completed': 0,
                'success_rate': 1.0,
                'total_tasks': 0,
                'recent_performance': []
            }
        
        performance = self.agent_performance[agent_name]
        performance['total_tasks'] += 1
        
        if 'error' not in result:
            performance['tasks_completed'] += 1
            performance['success_rate'] = performance['tasks_completed'] / performance['total_tasks']
            performance['recent_performance'].append('success')
        else:
            performance['recent_performance'].append('failure')
        
        # Stocker le r√©sultat
        self.completed_tasks[task] = result
        self.task_history.append({
            "task": task,
            "result": result,
            "completed_at": datetime.now().isoformat()
        })
        
        # Retirer la t√¢che du plan courant
        if task in self.task_ledger.current_plan:
            self.task_ledger.current_plan.remove(task)
        
        self.updated_at = datetime.now()

    def increment_stall(self):
        """Incr√©menter le compteur de blocages"""
        self.stall_count += 1
        self.updated_at = datetime.now()

    def reset_stall(self):
        """R√©initialiser le compteur de blocages"""
        self.stall_count = 0
        self.updated_at = datetime.now()

    def get_next_agent(self, task: str) -> str:
        """
        S√©lectionner le prochain agent bas√© sur les performances
        """
        best_agent = None
        best_score = -1

        for agent_name, perf in self.agent_performance.items():
            score = perf['success_rate'] * (perf['tasks_completed'] + 1)
            if score > best_score:
                best_score = score
                best_agent = agent_name

        return best_agent or "API Knowledge Agent"  # Agent par d√©faut

    def to_json(self) -> Dict[str, Any]:
        """Convertir en format JSON"""
        def convert_result(result):
            if hasattr(result, 'content'):
                return {
                    "content": result.content,
                    "content_type": str(result.content_type),
                    "event": str(result.event)
                }
            return result

        return {
            "task_ledger": self.task_ledger.to_json(),
            "completed_tasks": {
                task: {k: convert_result(v) for k, v in result.items()} 
                for task, result in self.completed_tasks.items()
            },
            "task_history": [
                {
                    "task": entry["task"],
                    "result": convert_result(entry["result"]),
                    "completed_at": entry["completed_at"]
                } 
                for entry in self.task_history
            ],
            "agent_performance": self.agent_performance,
            "stall_count": self.stall_count,
            "status": self.status
        }

class OrchestratorAgent:
    """
    Agent orchestrateur avanc√© avec d√©composition de t√¢ches
    """
    def __init__(
        self,
        model_id: str = "gpt-4o-mini", 
        debug_mode: bool = False,
        original_request: Optional[str] = None,
        api_key: Optional[str] = None,  
        enable_web_agent: bool = True,
        enable_api_knowledge_agent: bool = False,
        enable_data_analysis_agent: bool = False,
        enable_travel_planner: bool = False
    ):
        """
        Initialiser l'agent orchestrateur avec des agents sp√©cialis√©s
        
        Args:
            model_id (str): Identifiant du mod√®le OpenAI
            debug_mode (bool): Mode de d√©bogage
            original_request (Optional[str]): Requ√™te originale pour le TaskLedger
            api_key (Optional[str]): Cl√© API OpenAI personnalis√©e
            enable_web_agent (bool): Activer l'agent de recherche web
            enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
            enable_data_analysis_agent (bool): Activer l'agent d'analyse de donn√©es
            enable_travel_planner (bool): Activer l'agent de planification de voyage
        """
        # Configuration du mod√®le LLM
        self.llm_config = {
            "model": model_id,
            "temperature": 0.2
        }
        
        # Utiliser la cl√© API fournie ou celle de l'environnement
        openai_api_key = api_key or os.getenv('OPENAI_API_KEY')
        
        # Initialisation explicite du client OpenAI
        try:
            self.client = openai.OpenAI(api_key=openai_api_key)
        except Exception as e:
            logger.error(f"‚ùå Erreur d'initialisation du client OpenAI : {e}")
            self.client = None
        
        # Initialiser le mod√®le OpenAI
        self.model = self.client.chat.completions.create
        
        # Initialiser self.llm comme alias de self.model
        self.llm = self.model
        
        # Initialisation centralis√©e des agents
        self.agents = self._initialize_specialized_agents(
            enable_web_agent=enable_web_agent,
            enable_api_knowledge_agent=enable_api_knowledge_agent,
            enable_data_analysis_agent=enable_data_analysis_agent,
            enable_travel_planner=enable_travel_planner
        )
        
        # Initialisation du mode de d√©bogage
        self.debug_mode = debug_mode
        
        # Cr√©ation du TaskLedger initial
        self.task_ledger = self._create_task_ledger(original_request)
        
        # Cr√©er l'agent orchestrateur avec configuration simplifi√©e
        self.agent = self._create_orchestrator_agent(debug_mode)

    def _initialize_specialized_agents(
        self, 
        enable_web_agent: bool = False,
        enable_api_knowledge_agent: bool = False,
        enable_data_analysis_agent: bool = False,
        enable_travel_planner: bool = False
    ) -> Dict[str, Agent]:
        """
        Initialiser tous les agents sp√©cialis√©s de mani√®re dynamique
        
        Args:
            enable_web_agent (bool): Activer l'agent de recherche web
            enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
            enable_data_analysis_agent (bool): Activer l'agent d'analyse de donn√©es
            enable_travel_planner (bool): Activer l'agent de planification de voyage
        
        Returns:
            Dict[str, Agent]: Dictionnaire des agents disponibles
        """
        logger.info("ü§ñ Initialisation des agents sp√©cialis√©s")
        logger.info(f"üåê Web Agent: {enable_web_agent}")
        logger.info(f"üìö API Knowledge Agent: {enable_api_knowledge_agent}")
        logger.info(f"üìä Data Analysis Agent: {enable_data_analysis_agent}")
        logger.info(f"‚úàÔ∏è Travel Planner Agent: {enable_travel_planner}")
        
        agents = {}
        
        # Agent de recherche web
        if enable_web_agent:
            try:
                agents["web_search"] = get_web_searcher(
                    model_id=agent_settings.gpt_4,
                    debug_mode=False,
                    name="Web Search Agent"
                )
                logger.info("‚úÖ Agent de recherche web initialis√©")
            except Exception as e:
                logger.error(f"‚ùå Erreur d'initialisation de l'agent de recherche web : {e}")
        
        # Agent de connaissances API (√† impl√©menter)
        if enable_api_knowledge_agent:
            try:
                from agents.api_knowledge import get_api_knowledge_agent
                agents["api_knowledge"] = get_api_knowledge_agent(
                    debug_mode=False,
                    user_id=None,
                    session_id=None
                )
                logger.info("‚úÖ Agent de connaissances API initialis√©")
            except ImportError:
                logger.warning("‚ùå Agent de connaissances API non disponible")
        
        # Agent d'analyse de donn√©es (√† impl√©menter)
        if enable_data_analysis_agent:
            try:
                from agents.data_analysis import get_data_analysis_agent
                agents["data_analysis"] = get_data_analysis_agent(
                    debug_mode=False,
                    user_id=None,
                    session_id=None
                )
                logger.info("‚úÖ Agent d'analyse de donn√©es initialis√©")
            except ImportError:
                logger.warning("‚ùå Agent d'analyse de donn√©es non disponible")
        
        # Agent de planification de voyage (√† impl√©menter)
        if enable_travel_planner:
            try:
                from agents.travel import get_travel_planner_agent
                agents["travel_planner"] = get_travel_planner_agent(
                    debug_mode=False,
                    user_id=None,
                    session_id=None
                )
                logger.info("‚úÖ Agent de planification de voyage initialis√©")
            except ImportError:
                logger.warning("‚ùå Agent de planification de voyage non disponible")
        
        # Ajout d'un agent math√©matique par d√©faut
        agents["math_agent"] = Agent(
            instructions=[
                "Tu es un agent sp√©cialis√© dans les calculs math√©matiques.",
                "R√©ponds uniquement aux questions math√©matiques simples.",
                "Assure-toi de donner une r√©ponse pr√©cise et concise."
            ],
            name="Math Agent"
        )
        logger.info("‚úÖ Agent math√©matique par d√©faut initialis√©")
        
        # Log des agents disponibles
        logger.info(f"ü§ñ Agents initialis√©s : {list(agents.keys())}")
        
        return agents

    def _create_task_ledger(self, original_request: Optional[str] = None) -> TaskLedger:
        """
        Cr√©er un TaskLedger avec gestion robuste
        
        Args:
            original_request (Optional[str]): Requ√™te originale
        
        Returns:
            TaskLedger: Registre de t√¢ches initialis√©
        """
        return TaskLedger(
            original_request=original_request or "Requ√™te non sp√©cifi√©e",
            context={name: agent for name, agent in self.agents.items()}
        )

    def _create_orchestrator_agent(self, debug_mode: bool = False) -> Agent:
        """
        Cr√©er l'agent orchestrateur avec configuration simplifi√©e
        
        Args:
            debug_mode (bool): Mode de d√©bogage
        
        Returns:
            Agent: Agent orchestrateur configur√©
        """
        return Agent(
            # Utiliser la m√©thode create du client OpenAI
            llm=self.client.chat.completions.create,
            instructions=[
                "Tu es un agent d'orchestration avanc√© capable de d√©composer des t√¢ches complexes.",
                "√âtapes de travail :",
                "1. Analyser la requ√™te originale",
                "2. D√©composer la requ√™te en sous-t√¢ches pr√©cises et r√©alisables",
                "3. Attribuer chaque sous-t√¢che √† l'agent le plus appropri√©",
                "4. Suivre la progression de chaque sous-t√¢che",
                "5. Int√©grer et synth√©tiser les r√©sultats partiels",
                "6. Adapter dynamiquement le plan si n√©cessaire"
            ],
            # Ajouter le mode de d√©bogage si n√©cessaire
            debug_mode=debug_mode
        )

    def _get_task_decomposition_functions(self) -> List[Dict[str, Any]]:
        """
        D√©finir les fonctions d'appel pour la d√©composition de t√¢ches
        
        Returns:
            List[Dict[str, Any]]: Liste des d√©finitions de fonctions
        """
        return [
            {
                "name": "decompose_task",
                "description": "D√©composer une t√¢che complexe en sous-t√¢ches pr√©cises et ordonn√©es",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "subtasks": {
                            "type": "array",
                            "description": "Liste des sous-t√¢ches",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "task": {
                                        "type": "string", 
                                        "description": "Description pr√©cise de la sous-t√¢che"
                                    },
                                    "agent": {
                                        "type": "string", 
                                        "description": "Agent recommand√© pour la sous-t√¢che",
                                        "enum": [
                                            "Web Search Agent", 
                                            "API Knowledge Agent", 
                                            "Travel Planner Agent"
                                        ]
                                    },
                                    "priority": {
                                        "type": "string", 
                                        "description": "Priorit√© de la sous-t√¢che",
                                        "enum": ["haute", "moyenne", "basse"]
                                    }
                                },
                                "required": ["task", "agent", "priority"]
                            }
                        }
                    },
                    "required": ["subtasks"]
                }
            }
        ]

    def _get_agent_selection_functions(self) -> List[Dict[str, Any]]:
        """
        D√©finir les fonctions d'appel pour la s√©lection d'agents
        
        Args:
        
        Returns:
            List[Dict[str, Any]]: Liste des d√©finitions de fonctions
        """
        return [
            {
                "name": "select_best_agent",
                "description": "S√©lectionner l'agent le plus appropri√© pour une t√¢che donn√©e",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "task": {
                            "type": "string", 
                            "description": "Description de la t√¢che √† ex√©cuter"
                        },
                        "selected_agent": {
                            "type": "object",
                            "description": "Agent s√©lectionn√© avec son score de confiance",
                            "properties": {
                                "name": {
                                    "type": "string", 
                                    "description": "Nom de l'agent",
                                    "enum": [
                                        "Web Search Agent", 
                                        #"API Knowledge Agent", 

                                    ]
                                },
                                "confidence_score": {
                                    "type": "number", 
                                    "description": "Score de confiance (0.0-1.0)",
                                    "minimum": 0,
                                    "maximum": 1
                                },
                                "reasoning": {
                                    "type": "string", 
                                    "description": "Justification de la s√©lection"
                                }
                            },
                            "required": ["name", "confidence_score", "reasoning"]
                        }
                    },
                    "required": ["task", "selected_agent"]
                }
            }
        ]

    def should_decompose_task(self, user_request: str) -> bool:
        """
        Utiliser le LLM pour d√©terminer si la t√¢che n√©cessite une d√©composition
        
        Args:
            user_request (str): La requ√™te utilisateur originale
        
        Returns:
            bool: True si d√©composition n√©cessaire, False sinon
        """
        try:
            response = self.client.chat.completions.create(
                model=self.llm_config["model"],
                messages=[
                    {
                        "role": "system", 
                        "content": """
                        Tu es un expert en analyse de t√¢ches complexes.
                        
                        CRIT√àRES DE D√âCOMPOSITION :
                        - La t√¢che n√©cessite-t-elle vraiment d'√™tre divis√©e ?
                        - Pr√©sente-t-elle plusieurs dimensions ou √©tapes ?
                        - Requiert-elle diff√©rentes comp√©tences ou approches ?
                        
                        NE PAS D√âCOMPOSER pour :
                        - Requ√™tes simples et directes
                        - T√¢ches ne n√©cessitant qu'une seule action
                        - Demandes courtes et pr√©cises
                        
                        D√âCOMPOSER pour :
                        - Projets multi-√©tapes
                        - T√¢ches n√©cessitant planification
                        - Requ√™tes complexes avec plusieurs objectifs
                        
                        R√©ponds par "OUI" ou "NON" avec discernement
                        """
                    },
                    {
                        "role": "user", 
                        "content": f"Analyse cette requ√™te : {user_request}"
                    }
                ],
                max_tokens=10,
                temperature=0.2
            )
            
            llm_decision = response.choices[0].message.content.strip().upper()
            logger.info(f"D√©cision de d√©composition pour '{user_request}': {llm_decision}")
            
            return llm_decision == "OUI"
        
        except Exception as e:
            logger.error(f"Erreur lors de la d√©cision de d√©composition : {e}")
            return False

    def _select_best_agent(self, task: str) -> Agent:
        """
        S√©lectionner dynamiquement le meilleur agent pour une t√¢che donn√©e
        
        Args:
            task (str): La t√¢che √† ex√©cuter
        
        Returns:
            Agent: L'agent le plus appropri√©
        """
        # S√©lection par LLM
        try:
            # Utiliser l'API OpenAI pour classifier la t√¢che
            response = self.client.chat.completions.create(
                model=self.llm_config['model'],
                messages=[
                    {
                        "role": "system", 
                        "content": "Tu es un assistant qui aide √† s√©lectionner le bon agent parmi une liste."
                    },
                    {
                        "role": "user", 
                        "content": f"""
                        √âtant donn√© la t√¢che suivante, d√©termine quel agent serait le plus appropri√© :
                        
                        T√¢che : {task}
                        
                        Agents disponibles : {list(self.agents.keys())}
                        
                        R√©ponds uniquement avec le nom de l'agent le plus adapt√©.
                        """
                    }
                ],
                temperature=0.2,
                max_tokens=300
            )
            
            # Extraire et traiter la classification
            classification = response.choices[0].message.content.strip().lower()
            logger.info(f"üß† Classification de la t√¢che : {classification}")
            logger.info(f"üîç Agents disponibles : {list(self.agents.keys())}")

            # Convertir le nom en agent
            selected_agent_name = classification.lower().replace(' ', '_')
            selected_agent = self.agents.get(selected_agent_name)
            
            logger.info(f"üèÜ Agent s√©lectionn√© : {selected_agent_name}")
            return selected_agent
        
        except Exception as e:
            logger.error(f"‚ùå Erreur de s√©lection d'agent : {e}")
            return next(iter(self.agents.values()))

    def _publish_rabbitmq_message(self, queue_name: str, message: Dict[str, Any]) -> bool:
        """
        Publier un message dans une file RabbitMQ
        
        Args:
            queue_name (str): Nom de la file
            message (Dict[str, Any]): Message √† publier
        
        Returns:
            bool: True si la publication a r√©ussi, False sinon
        """
        try:
            # Importer pika de mani√®re s√©curis√©e
            import importlib
            import os
            from dotenv import load_dotenv
            
            # Charger les variables d'environnement
            load_dotenv()
            
            # R√©cup√©rer les param√®tres de connexion RabbitMQ
            rabbitmq_host = os.getenv('RABBITMQ_HOST', 'localhost')
            rabbitmq_port = int(os.getenv('RABBITMQ_PORT', 5672))
            rabbitmq_user = os.getenv('RABBITMQ_USER', '')
            rabbitmq_password = os.getenv('RABBITMQ_PASSWORD', '')
            
            # Importer pika
            pika = importlib.import_module('pika')
            
            # Configurer les param√®tres de connexion
            credentials = pika.PlainCredentials(rabbitmq_user, rabbitmq_password) if rabbitmq_user else None
            connection_params = pika.ConnectionParameters(
                host=rabbitmq_host,
                port=rabbitmq_port,
                credentials=credentials,
                connection_attempts=2,
                retry_delay=1
            )
            
            # √âtablir la connexion
            try:
                connection = pika.BlockingConnection(connection_params)
            except (pika.exceptions.AMQPConnectionError, ConnectionRefusedError) as conn_error:
                logger.warning(f"Impossible de se connecter √† RabbitMQ : {conn_error}")
                return False
            
            try:
                channel = connection.channel()
                
                # D√©clarer la queue si elle n'existe pas
                channel.queue_declare(queue=queue_name, durable=True)
                
                # Publier le message
                channel.basic_publish(
                    exchange='',
                    routing_key=queue_name,
                    body=json.dumps(message),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # Rendre le message persistant
                        content_type='application/json'
                    )
                )
                
                logger.info(f"Message publi√© dans la file {queue_name} sur {rabbitmq_host}:{rabbitmq_port}")
                return True
            
            except Exception as publish_error:
                logger.error(f"Erreur lors de la publication dans RabbitMQ : {publish_error}")
                return False
            
            finally:
                connection.close()
        
        except ImportError:
            logger.warning("La biblioth√®que pika n'est pas install√©e.")
            return False
        
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la publication RabbitMQ : {e}")
            return False

    async def decompose_task(self, user_request: str) -> TaskLedger:
        """
        D√©composer la requ√™te utilisateur en sous-t√¢ches avec function calling
        
        Args:
            user_request (str): La requ√™te utilisateur originale
        
        Returns:
            TaskLedger: Le registre de t√¢ches mis √† jour
        """
        try:
            # G√©n√©rer un ID unique pour la t√¢che
            task_id = str(uuid.uuid4())
            
            # D√©composer la t√¢che
            detailed_subtasks = self._generate_detailed_subtasks(user_request)
            
            # Mettre √† jour le TaskLedger
            self.task_ledger.current_plan = [
                subtask['description']
                for subtask in detailed_subtasks
            ]
            
            # Pr√©parer le message de progression pour RabbitMQ
            progress_message = {
                'task_id': task_id,
                'original_request': user_request,
                'total_subtasks': len(detailed_subtasks),
                'subtasks': detailed_subtasks,
                'status': 'started',
                'timestamp': datetime.now().isoformat()
            }
            
            # Publier le message dans la queue de progression
            self._publish_rabbitmq_message('queue_progress_task', progress_message)
            
            return self.task_ledger
        
        except Exception as e:
            logger.error(f"Erreur lors de la d√©composition de t√¢che : {e}")
            # Retourner le TaskLedger m√™me en cas d'erreur
            return self.task_ledger

    async def execute_task(
        self, 
        task_ledger: TaskLedger, 
        dev_mode: bool = False
    ) -> Dict[str, Any]:
        """
        Ex√©cuter les sous-t√¢ches de mani√®re unifi√©e
        
        Args:
            task_ledger (TaskLedger): Le registre de t√¢ches √† ex√©cuter
            dev_mode (bool): Mode d√©veloppement qui simule l'ex√©cution
        
        Returns:
            Dict[str, Any]: R√©sultats de l'ex√©cution des t√¢ches
        """
        try:
            # Initialiser le dictionnaire des r√©sultats
            task_results = {}
            
            # Ex√©cuter chaque sous-t√¢che
            for task_index, task in enumerate(task_ledger.current_plan, 1):
                try:
                    # S√©lectionner l'agent le plus appropri√©
                    selected_agent = self._select_best_agent(task)
                    
                    # Ex√©cuter la sous-t√¢che de mani√®re asynchrone
                    try:
                        # Utiliser arun() de mani√®re asynchrone
                        result = await selected_agent.arun(task)
                    except AttributeError:
                        # Lever une exception si arun() n'est pas disponible
                        raise RuntimeError(f"L'agent {selected_agent.name} ne supporte pas l'ex√©cution asynchrone")
                    
                    # Stocker le r√©sultat
                    task_results[task] = {
                        "agent": selected_agent.name,
                        "result": {
                            "content": result.content,
                            "content_type": result.content_type,
                            "event": result.event,
                            "messages": [
                                {
                                    "role": msg.role, 
                                    "content": msg.content
                                } for msg in result.messages
                            ] if result.messages else []
                        },
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    # Publier le message dans la queue de progression avec le r√©sultat
                    progress_message = {
                        "task_index": task_index,
                        "total_tasks": len(task_ledger.current_plan),
                        "task": task,
                        "agent": selected_agent.name,
                        "status": "completed",
                        "result": task_results[task]["result"],
                        "timestamp": datetime.now().isoformat()
                    }
                    self._publish_rabbitmq_message('queue_progress_task', progress_message)
                    
                    # Log de succ√®s
                    logger.info(f"‚úÖ Sous-t√¢che {task_index} termin√©e")
                    logger.info(f"üìä R√©sultat : {str(result)[:200]}...")
                    
                except Exception as task_error:
                    logger.error(f"‚ùå Erreur lors de l'ex√©cution de la sous-t√¢che {task_index} : {task_error}")
                    logger.error(traceback.format_exc())
                    
                    task_results[task] = {
                        'result': f"Erreur : {str(task_error)}",
                        'agent': 'error',
                        'traceback': traceback.format_exc()
                    }
            
            # Synth√©tiser les r√©sultats
            try:
                synthesized_result = await self._synthesize_results(list(task_results.values()))
                logger.info("üèÅ Ex√©cution de toutes les sous-t√¢ches termin√©e")
                logger.info(f"üìã R√©sultat synth√©tis√© : {str(synthesized_result)[:200]}...")
            except Exception as synthesis_error:
                logger.error(f"‚ùå Erreur lors de la synth√®se : {synthesis_error}")
                synthesized_result = "D√©sol√©, je n'ai pas pu synth√©tiser les r√©sultats."
            
            return {
                'task_results': task_results,
                'synthesized_result': synthesized_result
            }
        
        except Exception as e:
            logger.error(f"‚ùå Erreur globale lors de l'ex√©cution des t√¢ches : {e}")
            logger.error(traceback.format_exc())
            
            return {
                'task_results': {},
                'synthesized_result': "Erreur lors de l'ex√©cution des t√¢ches."
            }

    async def process_request(
        self, 
        user_request: str, 
        debug_mode: bool = False
    ) -> Dict[str, Any]:
        """
        Traiter une requ√™te de bout en bout
        
        Args:
            user_request (str): La requ√™te utilisateur
            debug_mode (bool): Mode de d√©bogage
        
        Returns:
            Dict[str, Any]: R√©sultats du traitement
        """
        try:
            # D√©cider si la t√¢che n√©cessite une d√©composition
            needs_decomposition = self.should_decompose_task(user_request)
            logger.info(f"üîç D√©composition requise : {needs_decomposition}")
            
            # S√©lectionner le mode de traitement
            if needs_decomposition:
                # D√©composer la t√¢che en sous-t√¢ches
                task_ledger = await self.decompose_task(user_request)
                
                # Ex√©cuter les sous-t√¢ches
                task_results = await self.execute_task(task_ledger)
                
                return task_results
            
            # Si pas de d√©composition, ex√©cuter directement
            selected_agent = self._select_best_agent(user_request)
            result = await selected_agent.arun(user_request)
            
            # Pr√©parer le message RabbitMQ
            task_result_message = {
                "task_index": 1,
                "total_tasks": 1,
                "task": user_request,
                "agent": selected_agent.name,
                "status": "completed",
                "result": {
                    "content": result.content,
                    "content_type": result.content_type,
                    "event": result.event,
                    "messages": [
                        {
                            "role": msg.role, 
                            "content": msg.content
                        } for msg in result.messages
                    ] if result.messages else []
                },
                "timestamp": datetime.now().isoformat()
            }
            
            # Publier le message dans la queue de progression
            self._publish_rabbitmq_message('queue_progress_task', task_result_message)
            
            # Log d√©taill√©
            logger.info(f"üìä R√©sultat complet : {result}")
            logger.info(f"üìù Contenu : {result.content}")
            
            return {
                'task_results': {user_request: result},
                'synthesized_result': result.content
            }
        
        except Exception as e:
            logger.error(f"Erreur lors du traitement de la requ√™te : {e}")
            logger.error(traceback.format_exc())
            
            return {
                'task_results': {},
                'synthesized_result': "Erreur lors du traitement de la requ√™te."
            }

    async def _synthesize_results(
        self, 
        subtask_results: List[Union[RunResponse, Dict, str]]
    ) -> str:
        """
        Synth√©tiser les r√©sultats de plusieurs sous-t√¢ches
        
        Args:
            subtask_results (List[Union[RunResponse, Dict, str]]): Liste des r√©sultats de sous-t√¢ches
        
        Returns:
            str: R√©sultat synth√©tis√©
        """
        logger.info("üèÅ D√©but de la synth√®se des r√©sultats")
        
        # Convertir les r√©sultats en format texte si n√©cessaire
        text_results = []
        for result in subtask_results:
            # G√©rer diff√©rents types de r√©sultats
            if isinstance(result, RunResponse):
                text_results.append(result.content)
            elif isinstance(result, dict):
                # Extraire le contenu du r√©sultat
                content = result.get('result', {})
                if isinstance(content, dict):
                    text_results.append(content.get('content', str(content)))
                else:
                    text_results.append(str(content))
            else:
                text_results.append(str(result))
        
        # Cas sp√©cial : r√©sultat unique
        if len(text_results) == 1:
            return text_results[0]
        
        # Utiliser l'agent orchestrateur pour synth√©tiser
        synthesis_prompt = f"""
        Synth√©tise les r√©sultats suivants de mani√®re concise et claire :
        
        {chr(10).join(text_results)}
        
        R√®gles pour la synth√®se :
        - Si plusieurs √©tapes, num√©rote et r√©sume chaque √©tape
        - Fournis un r√©sum√© final qui capture l'essence de tous les r√©sultats
        - Sois concis mais informatif
        """
        
        try:
            # Essayer d'utiliser arun() en premier
            synthesis_response = await self.agent.arun(synthesis_prompt)
            synthesized_result = synthesis_response.content
        except Exception:
            # Fallback √† la m√©thode synchrone
            synthesis_response = self.agent(synthesis_prompt)
            synthesized_result = synthesis_response.choices[0].message.content.strip()
        
        # Publier un message RabbitMQ avec la synth√®se
        try:
            synthesis_message = {
                "task_index": "synthesis",
                "total_tasks": len(subtask_results),
                "task": "Result Synthesis",
                "agent": "OrchestratorAgent",
                "status": "completed",
                "result": {
                    "content": synthesized_result,
                    "content_type": "text/plain",
                    "messages": []
                },
                "timestamp": datetime.now().isoformat()
            }
            self._publish_rabbitmq_message('queue_progress_task', synthesis_message)
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la publication de la synth√®se : {e}")
        
        logger.info(f"üìã R√©sultat synth√©tis√© : {synthesized_result}")
        return synthesized_result

    def _generate_detailed_subtasks(self, user_request: str) -> List[Dict[str, Any]]:
        """
        G√©n√©rer des sous-t√¢ches d√©taill√©es pour une requ√™te utilisateur
        
        Args:
            user_request (str): La requ√™te originale de l'utilisateur
        
        Returns:
            List[Dict[str, Any]]: Liste des sous-t√¢ches d√©taill√©es
        """
        try:
            # Pr√©parer le prompt pour la g√©n√©ration de sous-t√¢ches
            subtasks_prompt = f"""
            D√©compose la requ√™te suivante en sous-t√¢ches pr√©cises et r√©alisables :
            
            Requ√™te : {user_request}
            
            Instructions :
            - Divise la t√¢che en √©tapes concr√®tes et mesurables
            - Chaque sous-t√¢che doit √™tre claire et r√©alisable
            - Inclure des d√©tails sur l'objectif de chaque sous-t√¢che
            - Estimer un temps approximatif pour chaque sous-t√¢che
            
            Format de r√©ponse REQUIS (JSON strict) :
            {{
                "subtasks": [
                    {{
                        "task_id": "identifiant_unique",
                        "description": "Description d√©taill√©e de la sous-t√¢che",
                        "estimated_time": "Temps estim√©",
                        "priority": "haute/moyenne/basse",
                        "required_skills": ["comp√©tences requises"]
                    }}
                ]
            }}
            """
            
            # G√©n√©rer les sous-t√¢ches directement avec le client OpenAI
            response = self.client.chat.completions.create(
                model=self.llm_config.get('model', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "Tu es un expert en d√©composition de t√¢ches complexes."},
                    {"role": "user", "content": subtasks_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            
            # Extraire le contenu de la r√©ponse
            subtasks_response = response.choices[0].message.content
            
            # V√©rifier que la r√©ponse est une cha√Æne
            if not isinstance(subtasks_response, str):
                logger.error(f"La r√©ponse du mod√®le n'est pas une cha√Æne : {type(subtasks_response)}")
                raise ValueError("R√©ponse du mod√®le invalide")
            
            # Convertir la r√©ponse en liste de sous-t√¢ches
            try:
                subtasks_dict = json.loads(subtasks_response)
            except json.JSONDecodeError as json_err:
                logger.error(f"Erreur de d√©codage JSON : {json_err}")
                logger.error(f"R√©ponse re√ßue : {subtasks_response}")
                raise
            
            subtasks = subtasks_dict.get('subtasks', [])
            
            # V√©rifier que le format est correct
            if not isinstance(subtasks, list):
                raise ValueError("La r√©ponse n'est pas une liste de sous-t√¢ches")

            return subtasks
        
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des sous-t√¢ches : {e}")
            # Retourner une d√©composition par d√©faut si la g√©n√©ration √©choue
            return [
                {
                    "task_id": "task_1",
                    "description": f"Analyser la requ√™te : {user_request}",
                    "estimated_time": "1 heure",
                    "priority": "haute",
                    "required_skills": ["compr√©hension", "analyse"]
                }
            ]

async def process_user_request(
    user_request: str, 
    debug_mode: bool = False
) -> Dict[str, Any]:
    """
    Traiter une requ√™te utilisateur de mani√®re asynchrone avec l'orchestrateur
    
    Args:
        user_request (str): La requ√™te de l'utilisateur
        debug_mode (bool): Mode de d√©bogage
    
    Returns:
        Dict[str, Any]: R√©sultat du traitement de la requ√™te
    """
    try:
        orchestrator = OrchestratorAgent(
            debug_mode=debug_mode, 
            original_request=user_request
        )
        result = await orchestrator.process_request(
            user_request=user_request,
            debug_mode=debug_mode
        )
        
        # Extraction de la synth√®se
        synthesized_result = result.get('synthesized_result', '')
        
        # D√©termination de l'agent utilis√©
        agent_used = 'Multi-Purpose Intelligence Team'
        task_results = result.get('task_results', {})
        
        # Cas avec plusieurs t√¢ches : utiliser la synth√®se
        if len(task_results) > 1:
            agent_used = list(task_results.values())[0].get('agent', agent_used)
        
        # Cas avec une seule t√¢che : extraire le contenu
        elif len(task_results) == 1:
            first_task = list(task_results.keys())[0]
            task_result = task_results[first_task]
            
            # Si c'est un RunResponse
            if hasattr(task_result, 'content'):
                synthesized_result = task_result.content
                agent_used = task_result.name if hasattr(task_result, 'name') else agent_used
            
            # Si c'est un dictionnaire
            elif isinstance(task_result, dict):
                # Essayer d'extraire le contenu de diff√©rentes mani√®res
                if 'result' in task_result:
                    result_content = task_result['result']
                    if isinstance(result_content, dict) and 'content' in result_content:
                        synthesized_result = result_content['content']
                    elif isinstance(result_content, str):
                        synthesized_result = result_content
                
                agent_used = task_result.get('agent', agent_used)
        
        return {
            'query': user_request,
            'result': synthesized_result,
            'agent_used': agent_used,
            'metadata': {},
            'error': None
        }
    except Exception as e:
        logger.error(f"Erreur lors du traitement de la requ√™te : {e}")
        logger.error(traceback.format_exc())
        return {
            'query': user_request,
            'result': '',
            'agent_used': 'Multi-Purpose Intelligence Team',
            'metadata': {},
            'error': str(e)
        }

def get_orchestrator_agent(
    model_id: str = "gpt-4o-mini",
    enable_web_agent: bool = True,
    enable_api_knowledge_agent: bool = False,
    enable_data_analysis_agent: bool = False,
    enable_travel_planner: bool = False
) -> OrchestratorAgent:
    """
    Cr√©er un agent orchestrateur avec configuration personnalisable
    
    Args:
        model_id (str): Identifiant du mod√®le OpenAI
        enable_web_agent (bool): Activer l'agent de recherche web
        enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
        enable_data_analysis_agent (bool): Activer l'agent d'analyse de donn√©es
        enable_travel_planner (bool): Activer l'agent de planification de voyage
    
    Returns:
        OrchestratorAgent: Agent orchestrateur configur√©
    """
    return OrchestratorAgent(
        model_id=model_id,
        enable_web_agent=enable_web_agent,
        enable_api_knowledge_agent=enable_api_knowledge_agent,
        enable_data_analysis_agent=enable_data_analysis_agent,
        enable_travel_planner=enable_travel_planner
    )

# Exemple d'utilisation
if __name__ == "__main__":
    import logging
    import json

    # Configuration du logging
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    def extract_content(chunk):
        """
        Extraire le contenu textuel d'un chunk de diff√©rents types
        """
        # Si c'est un tuple
        if isinstance(chunk, tuple):
            # Si le tuple a plus d'un √©l√©ment, prendre le deuxi√®me
            if len(chunk) > 1:
                chunk = chunk[1]
            else:
                chunk = ""
        
        # Si c'est une liste, convertir en cha√Æne
        if isinstance(chunk, list):
            chunk = " ".join(str(item) for item in chunk)
        
        # Convertir en cha√Æne si ce n'est pas d√©j√† une cha√Æne
        return str(chunk)

    def test_orchestrator():
        # Cr√©er l'agent orchestrateur sans agent web
        orchestrator = get_orchestrator_agent(
            enable_web_agent=True  # Activer la recherche web
        )
        
        # Exemples de requ√™tes de test
        test_requests = [
            "Faire une analyse comparative des performances des startups tech en 2024"
        ]
        
        for request in test_requests:
            print(f"\nüöÄ Traitement de la requ√™te : {request}")
            
            # Utiliser la g√©n√©ration de r√©ponse directe
            try:
                # R√©cup√©rer le g√©n√©rateur de r√©ponse
                resp = orchestrator.run(request)
                
                # Collecter et afficher les r√©sultats par morceaux
                print("\nüìä R√©sultats :")
                full_result = ""
                for chunk in resp:
                    # Extraire le contenu du chunk
                    chunk_content = extract_content(chunk)
                    
                    print(chunk_content, end='', flush=True)
                    full_result += chunk_content
                
                print("\n\nüîç R√©sum√© :")
                print(f"Longueur totale de la r√©ponse : {len(full_result)} caract√®res")
            except Exception as e:
                print(f"Erreur lors de l'ex√©cution : {e}")
                import traceback
                traceback.print_exc()
    
    # Ex√©cuter le test
    test_orchestrator()