import os
import sys
import asyncio
import logging
import traceback
import uuid
import json
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union, Callable
from dataclasses import dataclass, field
import time

import openai
from phi.agent import RunResponse, Agent
from phi.model.openai import OpenAIChat

from agents.web import get_web_searcher
from agents.agent_base import get_agent_base



from agents.settings import agent_settings
from agents.orchestrator_prompts import (
    get_task_decomposition_prompt,
    get_task_execution_prompt,
    get_task_synthesis_prompt
)

from utils.colored_logging import get_colored_logger



agent_memory_file: str = "orchestrator_agent_memory.db"
agent_storage_file: str = "orchestrator_agent_sessions.db"


# Configuration du logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Ajout d'un handler de console si n√©cessaire
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# Ajouter le r√©pertoire parent au PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

# Ajouter le handler au logger s'il n'est pas d√©j√† pr√©sent
if not logger.handlers:
    logger.addHandler(console_handler)


@dataclass
class TaskLedger:
    """
    Registre pour g√©rer les faits et le plan de t√¢ches
    """
    original_request: str
    task_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    initial_plan: List[str] = field(default_factory=list)
    _current_plan: List[str] = field(default_factory=list, repr=False)
    facts: Dict[str, Any] = field(default_factory=lambda: {
        "verified": [],  # Faits v√©rifi√©s
        "to_lookup": [], # Faits √† rechercher
        "derived": [],   # Faits d√©riv√©s (calculs/logique)
        "guesses": []    # Suppositions √©duqu√©es
    })
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    context: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not isinstance(self._current_plan, list):
            if isinstance(self._current_plan, dict):
                self._current_plan = list(self._current_plan.values())
            elif isinstance(self._current_plan, str):
                self._current_plan = [self._current_plan]
            else:
                self._current_plan = [str(self._current_plan)] if self._current_plan is not None else []

    @property
    def current_plan(self) -> List[str]:
        return self._current_plan

    @current_plan.setter
    def current_plan(self, value: Union[List[str], Dict[str, Any], str, Any]):
        if not isinstance(value, list):
            if isinstance(value, dict):
                value = list(value.values())
            elif isinstance(value, str):
                value = [value]
            else:
                value = [str(value)] if value is not None else []
        self._current_plan = value
        self.updated_at = datetime.now()

    def add_fact(self, fact: str, fact_type: str = "verified"):
        """Ajouter un fait au registre"""
        if fact_type in self.facts:
            if fact not in self.facts[fact_type]:
                self.facts[fact_type].append(fact)
                self.updated_at = datetime.now()

    def add_task(self, task: str):
        """Ajouter une t√¢che au plan courant"""
        if task not in self._current_plan:
            self._current_plan.append(task)
            self.updated_at = datetime.now()

    def register_agent(self, agent: Agent, agent_name: str):
        """Enregistrer un agent dans le registre"""
        self.context[agent_name] = agent

    def to_json(self) -> Dict[str, Any]:
        """
        Convertir le TaskLedger en format JSON
        """
        return {
            "task_id": self.task_id,
            "original_request": self.original_request,
            "initial_plan": self.initial_plan,
            "current_plan": self.current_plan,
            "facts": self.facts,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "context": self.context
        }

@dataclass
class ProgressLedger:
    """
    Registre pour suivre la progression et g√©rer les blocages
    """
    task_ledger: TaskLedger
    completed_tasks: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    task_history: List[Dict[str, Any]] = field(default_factory=list)
    agent_performance: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    stall_count: int = 0
    max_stalls: int = 2
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    status: str = "pending"

    def is_task_complete(self) -> bool:
        """V√©rifie si toutes les t√¢ches sont termin√©es"""
        return len(self.task_ledger.current_plan) == 0

    def is_making_progress(self) -> bool:
        """V√©rifie si des progr√®s sont r√©alis√©s"""
        if not self.task_history:
            return True
        last_update = datetime.fromisoformat(self.task_history[-1]["completed_at"])
        time_since_last_update = datetime.now() - last_update
        return time_since_last_update.seconds < 300  # 5 minutes

    def is_stalled(self) -> bool:
        """V√©rifie si l'ex√©cution est bloqu√©e"""
        return self.stall_count >= self.max_stalls

    def complete_task(self, task: str, result: Dict[str, Any]):
        """Marquer une t√¢che comme termin√©e"""
        agent_name = result.get('agent', 'unknown')
        
        # Mettre √† jour les performances de l'agent
        if agent_name not in self.agent_performance:
            self.agent_performance[agent_name] = {
                'tasks_completed': 0,
                'success_rate': 1.0,
                'total_tasks': 0,
                'recent_performance': []
            }
        
        performance = self.agent_performance[agent_name]
        performance['total_tasks'] += 1
        
        if 'error' not in result:
            performance['tasks_completed'] += 1
            performance['success_rate'] = performance['tasks_completed'] / performance['total_tasks']
            performance['recent_performance'].append('success')
        else:
            performance['recent_performance'].append('failure')
        
        # Stocker le r√©sultat
        self.completed_tasks[task] = result
        self.task_history.append({
            "task": task,
            "result": result,
            "completed_at": datetime.now().isoformat()
        })
        
        # Retirer la t√¢che du plan courant
        if task in self.task_ledger.current_plan:
            self.task_ledger.current_plan.remove(task)
        
        self.updated_at = datetime.now()

    def increment_stall(self):
        """Incr√©menter le compteur de blocages"""
        self.stall_count += 1
        self.updated_at = datetime.now()

    def reset_stall(self):
        """R√©initialiser le compteur de blocages"""
        self.stall_count = 0
        self.updated_at = datetime.now()

    def get_next_agent(self, task: str) -> str:
        """
        S√©lectionner le prochain agent bas√© sur les performances
        """
        best_agent = None
        best_score = -1

        for agent_name, perf in self.agent_performance.items():
            score = perf['success_rate'] * (perf['tasks_completed'] + 1)
            if score > best_score:
                best_score = score
                best_agent = agent_name

        return best_agent or "API Knowledge Agent"  # Agent par d√©faut

    def to_json(self) -> Dict[str, Any]:
        """Convertir en format JSON"""
        def convert_result(result):
            if hasattr(result, 'content'):
                return {
                    "content": result.content,
                    "content_type": str(result.content_type),
                    "event": str(result.event)
                }
            return result

        return {
            "task_ledger": self.task_ledger.to_json(),
            "completed_tasks": {
                task: {k: convert_result(v) for k, v in result.items()} 
                for task, result in self.completed_tasks.items()
            },
            "task_history": [
                {
                    "task": entry["task"],
                    "result": convert_result(entry["result"]),
                    "completed_at": entry["completed_at"]
                } 
                for entry in self.task_history
            ],
            "agent_performance": self.agent_performance,
            "stall_count": self.stall_count,
            "status": self.status
        }

class OrchestratorAgent:
    """
    Agent orchestrateur avanc√© avec d√©composition de t√¢ches
    """
    def __init__(
        self, 
        model_id: str = "gpt-4o-mini", 
        debug_mode: bool = False,
        original_request: Optional[str] = None,
        api_key: Optional[str] = None,  
        enable_web_agent: bool = True,
        enable_api_knowledge_agent: bool = False,
        enable_agent_base: bool = True,
        # Param√®tres de configuration
        name: str = "Orchestrator Agent",
        instructions: Optional[List[str]] = None,
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
        model: Optional[OpenAIChat] = None,
        **kwargs
    ):
        """
        Initialiser l'agent orchestrateur avec des agents sp√©cialis√©s
        
        Args:
            user_id (Optional[str]): Identifiant unique de l'utilisateur
            session_id (Optional[str]): Identifiant de session pour le suivi
        """
        # Pr√©parer les instructions par d√©faut
        default_instructions = [
            "ü§ñ Agent d'Orchestration Intelligent pour la R√©solution Adaptative des Requ√™tes",
            "Objectif Principal : Traiter Efficacement Chaque Requ√™te avec une Approche Multi-Strat√©gique",
            "",
            "üîç Strat√©gies de Traitement :",
            "1. Analyse Contextuelle Approfondie :",
            "   - √âvaluer la requ√™te initiale",
            "   - Consulter la m√©moire utilisateur pour enrichissement",
            "   - D√©terminer le mode de traitement optimal",
            "",
            "2. Modes de R√©solution :",
            "   a) Traitement Direct :",
            "      - R√©soudre imm√©diatement si la requ√™te est simple",
            "      - Utiliser les connaissances existantes",
            "   b) Enrichissement Contextuel :",
            "      - Consulter et int√©grer les informations de la m√©moire utilisateur",
            "      - Affiner et compl√©ter la requ√™te initiale",
            "   c) D√©composition Strat√©gique :",
            "      - Fragmenter les t√¢ches complexes en sous-t√¢ches pr√©cises",
            "      - Attribuer chaque sous-t√¢che √† l'agent sp√©cialis√©",
            "",
            "3. Routage Intelligent :",
            "   - S√©lectionner dynamiquement l'agent le plus adapt√© :",
            "     * Analyse s√©mantique de la requ√™te",
            "     * Correspondance avec les comp√©tences des agents disponibles",
            "     * √âvaluation de la complexit√© et du contexte",
            "     * Capacit√© √† cr√©er ou adapter des agents dynamiquement",
            "",
            "üß† Principes Op√©rationnels :",
            "- √ätre le point d'entr√©e unique et adaptatif pour toutes les requ√™tes",
            "- Maximiser la pr√©cision et la pertinence de la r√©ponse",
            "- Maintenir une tra√ßabilit√© compl√®te du processus de r√©solution",
            "- S'adapter dynamiquement aux diff√©rents types et complexit√©s de demandes"
        ]
        instructions = instructions or default_instructions

        # Initialisation des identifiants
        self.user_id = user_id or str(uuid.uuid4())
        self.session_id = session_id or str(uuid.uuid4())

        # Initialisation du client OpenAI
        openai_api_key = api_key or os.getenv('OPENAI_API_KEY')
        
        # Cr√©er une m√©thode de fallback pour les appels OpenAI
        def create_openai_client():
            try:
                return openai.OpenAI(api_key=openai_api_key)
            except Exception as e:
                logger.error(f"‚ùå Erreur d'initialisation du client OpenAI : {e}")
                return None
        
        # Stocker le client OpenAI comme attribut priv√©
        self._openai_client = create_openai_client()
        
        # M√©thode de fallback pour les appels de compl√©tion
        def fallback_chat_completions_create(*args, **kwargs):
            if self._openai_client:
                return self._openai_client.chat.completions.create(*args, **kwargs)
            else:
                raise ValueError("Client OpenAI non initialis√©")
        
        # Stocker la m√©thode de cr√©ation de compl√©tion
        self._llm_create = fallback_chat_completions_create

        # Initialisation du mod√®le
        self.model_id = model_id
        self.model = model or OpenAIChat(model=model_id, temperature=0.2)
        self.debug_mode = debug_mode
        
        # Initialisation des agents sp√©cialis√©s
        self.agents = self._initialize_specialized_agents(
            enable_web_agent=enable_web_agent,
            enable_api_knowledge_agent=enable_api_knowledge_agent,
            enable_agent_base=enable_agent_base,

        )
        
        # Cr√©ation du TaskLedger initial
        self.task_ledger = self._create_task_ledger(original_request)
        
        # Cr√©er l'agent orchestrateur avec configuration simplifi√©e
        self.agent = self._create_orchestrator_agent(debug_mode)

    def run(self, task: str) -> RunResponse:
        """
        M√©thode run standard pour l'Agent Phidata
        D√©l√®gue au processus de traitement de requ√™te existant
        """
        try:
            result = self.process_request(task, debug_mode=self.debug_mode)
            return RunResponse(
                content=result.get('final_result', 'Aucun r√©sultat'),
                content_type='text',
                metadata=result
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'ex√©cution de l'orchestrateur : {e}")
            return RunResponse(
                content=f"Erreur : {str(e)}",
                content_type='error'
            )

    def _initialize_specialized_agents(
        self, 
        enable_web_agent: bool = True,
        enable_api_knowledge_agent: bool = False,
        enable_agent_base: bool = True,
    ) -> Dict[str, Any]:
        """
        Initialiser les agents sp√©cialis√©s
        
        Args:
            enable_web_agent (bool): Activer l'agent de recherche web
            enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
            enable_data_analysis_agent (bool): Activer l'agent d'analyse de donn√©es
            enable_travel_planner (bool): Activer l'agent de planification de voyage
        
        Returns:
            Dict[str, Any]: Dictionnaire des agents initialis√©s
        """
        logger.info("ü§ñ Initialisation des agents sp√©cialis√©s")
        logger.info(f"üåê Web Agent: {enable_web_agent}")
        logger.info(f"üìö API Knowledge Agent: {enable_api_knowledge_agent}")
        logger.info(f"üìä Base Agent : {enable_agent_base}")
        
        
        agents = {}
        
        # Initialisation de l'agent de recherche web
        if enable_web_agent:
            try:
                web_agent = get_web_searcher()
                agents_with_descriptions = {
                    'web_agent': {
                        'agent': web_agent,
                        'description': "Agent sp√©cialis√© dans la recherche et la collecte d'informations sur le web. Id√©al pour les requ√™tes n√©cessitant des donn√©es actualis√©es ou des recherches en ligne."
                    },
                }
                agents.update(agents_with_descriptions)
                logger.info("‚úÖ Agent de recherche web initialis√© avec succ√®s")
            except Exception as e:
                logger.error(f"‚ùå Erreur d'initialisation de l'agent de recherche web : {e}")
                logger.debug(traceback.format_exc())
        
        # Placeholder pour les autres agents (√† impl√©menter si n√©cessaire)
        if enable_api_knowledge_agent:
            logger.warning("üöß API Knowledge Agent non impl√©ment√©")
        
        if enable_agent_base:
            try:
                agent_base = get_agent_base()
                agents_with_descriptions = {
                    'agent_base': {
                        'agent': agent_base,
                        'description': "Agent conversationnel polyvalent capable de r√©pondre √† une large vari√©t√© de questions. Adapt√© aux t√¢ches g√©n√©rales n√©cessitant compr√©hension et analyse."
                    }
                }
                agents.update(agents_with_descriptions)
                logger.info("‚úÖ Agent de base initialis√© avec succ√®s")
            except Exception as e:
                logger.error(f"‚ùå Erreur d'initialisation de l'agent de base : {e}")
                logger.debug(traceback.format_exc())
        
        return agents

    def _create_task_ledger(self, original_request: Optional[str] = None) -> TaskLedger:
        """
        Cr√©er un TaskLedger avec gestion robuste
        
        Args:
            original_request (Optional[str]): Requ√™te originale
        
        Returns:
            TaskLedger: Registre de t√¢ches initialis√©
        """
        return TaskLedger(
            original_request=original_request or "Requ√™te non sp√©cifi√©e",
            context={name: agent for name, agent in self.agents.items()}
        )

    def _create_orchestrator_agent(self, debug_mode: bool = False) -> Agent:
        """
        Cr√©er l'agent orchestrateur avec configuration simplifi√©e
        
        Args:
            debug_mode (bool): Mode de d√©bogage
        
        Returns:
            Agent: Agent orchestrateur configur√©
        """
        return Agent(
            # Utiliser la m√©thode create du client OpenAI
            llm=self._openai_client.chat.completions.create,
            instructions=[
                "ü§ñ Agent d'Orchestration Intelligent pour la R√©solution Adaptative des Requ√™tes",
                "Objectif Principal : Traiter Efficacement Chaque Requ√™te avec une Approche Multi-Strat√©gique",
                "",
                "üîç Strat√©gies de Traitement :",
                "1. Analyse Contextuelle Approfondie :",
                "   - √âvaluer la requ√™te initiale",
                "   - Consulter la m√©moire utilisateur pour enrichissement",
                "   - D√©terminer le mode de traitement optimal",
                "",
                "2. Modes de R√©solution :",
                "   a) Traitement Direct :",
                "      - R√©soudre imm√©diatement si la requ√™te est simple",
                "      - Utiliser les connaissances existantes",
                "   b) Enrichissement Contextuel :",
                "      - Consulter et int√©grer les informations de la m√©moire utilisateur",
                "      - Affiner et compl√©ter la requ√™te initiale",
                "   c) D√©composition Strat√©gique :",
                "      - Fragmenter les t√¢ches complexes en sous-t√¢ches pr√©cises",
                "      - Attribuer chaque sous-t√¢che √† l'agent sp√©cialis√©",
                "",
                "3. Routage Intelligent :",
                "   - S√©lectionner dynamiquement l'agent le plus adapt√© :",
                "     * Analyse s√©mantique de la requ√™te",
                "     * Correspondance avec les comp√©tences des agents disponibles",
                "     * √âvaluation de la complexit√© et du contexte",
                "     * Capacit√© √† cr√©er ou adapter des agents dynamiquement",
                "",
                "üß† Principes Op√©rationnels :",
                "- √ätre le point d'entr√©e unique et adaptatif pour toutes les requ√™tes",
                "- Maximiser la pr√©cision et la pertinence de la r√©ponse",
                "- Maintenir une tra√ßabilit√© compl√®te du processus de r√©solution",
                "- S'adapter dynamiquement aux diff√©rents types et complexit√©s de demandes"
            ],
            # Ajouter le mode de d√©bogage si n√©cessaire
            debug_mode=debug_mode
        )

    def _get_task_decomposition_functions(self) -> List[Dict[str, Any]]:
        """
        D√©finir les fonctions d'appel pour la d√©composition de t√¢ches
        
        Returns:
            List[Dict[str, Any]]: Liste des d√©finitions de fonctions
        """
        return [
            {
                "name": "decompose_task",
                "description": "D√©composer une t√¢che complexe en sous-t√¢ches pr√©cises et ordonn√©es",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "subtasks": {
                            "type": "array",
                            "description": "Liste des sous-t√¢ches",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "task": {
                                        "type": "string", 
                                        "description": "Description pr√©cise de la sous-t√¢che"
                                    },
                                    "agent": {
                                        "type": "string", 
                                        "description": "Agent recommand√© pour la sous-t√¢che",
                                        "enum": [
                                            "Web Search Agent", 
                                            "API Knowledge Agent", 
                                            "Agent Base"
                                        ]
                                    },
                                    "priority": {
                                        "type": "string", 
                                        "description": "Priorit√© de la sous-t√¢che",
                                        "enum": ["haute", "moyenne", "basse"]
                                    }
                                },
                                "required": ["task", "agent", "priority"]
                            }
                        }
                    },
                    "required": ["subtasks"]
                }
            }
        ]

    def _get_agent_selection_functions(self) -> List[Dict[str, Any]]:
        """
        D√©finir les fonctions d'appel pour la s√©lection d'agents
        
        Args:
        
        Returns:
            List[Dict[str, Any]]: Liste des d√©finitions de fonctions
        """
        return [
            {
                "name": "select_best_agent",
                "description": "S√©lectionner l'agent le plus appropri√© pour une t√¢che donn√©e",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "task": {
                            "type": "string", 
                            "description": "Description de la t√¢che √† ex√©cuter"
                        },
                        "selected_agent": {
                            "type": "object",
                            "description": "Agent s√©lectionn√© avec son score de confiance",
                            "properties": {
                                "name": {
                                    "type": "string", 
                                    "description": "Nom de l'agent",
                                    "enum": [
                                        "Web Search Agent", 
                                        #"API Knowledge Agent", 

                                    ]
                                },
                                "confidence_score": {
                                    "type": "number", 
                                    "description": "Score de confiance (0.0-1.0)",
                                    "minimum": 0,
                                    "maximum": 1
                                },
                                "reasoning": {
                                    "type": "string", 
                                    "description": "Justification de la s√©lection"
                                }
                            },
                            "required": ["name", "confidence_score", "reasoning"]
                        }
                    },
                    "required": ["task", "selected_agent"]
                }
            }
        ]

    def should_decompose_task(self, user_request: str) -> bool:
        """
        Utiliser le LLM pour d√©terminer si la t√¢che n√©cessite une d√©composition
        
        Args:
            user_request (str): La requ√™te utilisateur originale
        
        Returns:
            bool: True si d√©composition n√©cessaire, False sinon
        """
        logger.info(f"üîç Analyse de d√©composition pour la requ√™te : '{user_request}'")
        
        try:
            # Utiliser l'API OpenAI pour d√©terminer la n√©cessit√© de d√©composition
            response = self._openai_client.chat.completions.create(
                model=self.model_id,
                messages=[
                    {
                        "role": "system", 
                        "content": """
                        Tu es un expert en analyse de t√¢ches complexes.
                        
                        CRIT√àRES DE D√âCOMPOSITION :
                        - La t√¢che n√©cessite-t-elle vraiment d'√™tre divis√©e ?
                        - Pr√©sente-t-elle plusieurs dimensions ou √©tapes ?
                        - Requiert-elle diff√©rentes comp√©tences ou approches ?
                        
                        NE PAS D√âCOMPOSER pour :
                        - Requ√™tes simples et directes
                        - T√¢ches ne n√©cessitant qu'une seule action
                        - Demandes courtes et pr√©cises
                        
                        D√âCOMPOSER pour :
                        - Projets multi-√©tapes
                        - T√¢ches n√©cessitant planification
                        - Requ√™tes complexes avec plusieurs objectifs
                        
                        R√©ponds par "OUI" ou "NON" avec discernement
                        """
                    },
                    {
                        "role": "user", 
                        "content": f"Analyse cette requ√™te : {user_request}"
                    }
                ],
                max_tokens=10,
                temperature=0.2
            )
            
            llm_decision = response.choices[0].message.content.strip().upper()
            logger.info(f"D√©cision de d√©composition pour '{user_request}': {llm_decision}")
            
            return llm_decision == "OUI"
        
        except Exception as e:
            logger.error(f"Erreur lors de la d√©cision de d√©composition : {e}")
            return False

    def _select_best_agent(self, task: str) -> Agent:
        """
        S√©lectionner dynamiquement le meilleur agent pour une t√¢che donn√©e
        
        Args:
            task (str): La t√¢che √† ex√©cuter
        
        Returns:
            Agent: L'agent le plus appropri√©
        """
        # S√©lection par LLM
        try:
            # Pr√©parer les descriptions des agents pour l'analyse
            agents_descriptions = "\n".join([
                f"- {name}: {agent_info.get('description', 'Pas de description disponible')}"
                for name, agent_info in self.agents.items()
            ])
            
            messages=[
                    {
                        "role": "system", 
                        "content": "Tu es un assistant qui aide √† s√©lectionner le bon agent parmi une liste."
                    },
                    {
                        "role": "user", 
                        "content": f"""
                        Je dois s√©lectionner le meilleur agent pour la t√¢che suivante : "{task}"
                        
                        Voici les agents disponibles :
                        {agents_descriptions}
                        
                        Crit√®res de s√©lection :
                        - Analyse la description de chaque agent
                        - √âvalue sa pertinence par rapport √† la t√¢che
                        - Prends en compte la sp√©cialisation de l'agent
                        
                        R√©ponds uniquement avec le nom de l'agent le plus adapt√©.
                        """
                    }
                ]
            response = self._openai_client.chat.completions.create(
                model=self.model_id,
                messages=messages,
                temperature=0.2,
                max_tokens=300
            )
            
            # Extraire et traiter la classification
            classification = response.choices[0].message.content.strip().lower()
            logger.info(f"üß† Classification de la t√¢che : {classification}")
            logger.debug(f"üß† messages : {messages}")
            logger.info(f"üîç Agents disponibles : {list(self.agents.keys())}")

            # Convertir le nom en agent
            selected_agent_name = classification.replace(' ', '_').lower()
            
            # V√©rifier si l'agent existe
            if selected_agent_name in self.agents:
                selected_agent = self.agents[selected_agent_name]['agent']
                logger.info(f"üéØ Agent s√©lectionn√© : {selected_agent_name}")
                return selected_agent
            
            # Fallback : utiliser l'agent de base si aucun agent n'est trouv√©
            logger.warning(f"‚ö†Ô∏è Aucun agent trouv√© pour '{selected_agent_name}'. Utilisation de l'agent de base.")
            return self.agents['agent_base']['agent']
        
        except Exception as e:
            logger.error(f"‚ùå Erreur de s√©lection d'agent : {e}")
            return next(iter(self.agents.values()))

    def _publish_rabbitmq_message(self, queue_name: str, message: Dict[str, Any]) -> bool:
        """
        Publier un message dans une file RabbitMQ
        
        Args:
            queue_name (str): Nom de la file
            message (Dict[str, Any]): Message √† publier
        
        Returns:
            bool: True si la publication a r√©ussi, False sinon
        """
        try:
            # Importer pika de mani√®re s√©curis√©e
            import importlib
            import os
            from dotenv import load_dotenv
            
            # Charger les variables d'environnement
            load_dotenv()
            
            # R√©cup√©rer les param√®tres de connexion RabbitMQ
            rabbitmq_host = os.getenv('RABBITMQ_HOST', 'localhost')
            rabbitmq_port = int(os.getenv('RABBITMQ_PORT', 5672))
            rabbitmq_user = os.getenv('RABBITMQ_USER', '')
            rabbitmq_password = os.getenv('RABBITMQ_PASSWORD', '')
            
            # Importer pika
            pika = importlib.import_module('pika')
            
            # Configurer les param√®tres de connexion
            credentials = pika.PlainCredentials(rabbitmq_user, rabbitmq_password) if rabbitmq_user else None
            connection_params = pika.ConnectionParameters(
                host=rabbitmq_host,
                port=rabbitmq_port,
                credentials=credentials,
                connection_attempts=2,
                retry_delay=1
            )
            
            # √âtablir la connexion
            try:
                connection = pika.BlockingConnection(connection_params)
            except (pika.exceptions.AMQPConnectionError, ConnectionRefusedError) as conn_error:
                logger.warning(f"Impossible de se connecter √† RabbitMQ : {conn_error}")
                return False
            
            try:
                channel = connection.channel()
                
                # D√©clarer la queue si elle n'existe pas
                channel.queue_declare(queue=queue_name, durable=True)
                
                # Publier le message
                channel.basic_publish(
                    exchange='',
                    routing_key=queue_name,
                    body=json.dumps(message),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # Rendre le message persistant
                        content_type='application/json'
                    )
                )
                
                logger.info(f"Message RabbitMQ publi√© dans la file {queue_name} sur {rabbitmq_host}:{rabbitmq_port}")
                return True
            
            except Exception as publish_error:
                logger.error(f"Erreur lors de la publication dans RabbitMQ : {publish_error}")
                return False
            
            finally:
                connection.close()
        
        except ImportError:
            logger.warning("La biblioth√®que pika n'est pas install√©e.")
            return False
        
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la publication RabbitMQ : {e}")
            return False

    def _create_task_message(
        self, 
        task_type: str, 
        request_id: str, 
        sub_task_id: Optional[str] = None,
        original_request: Optional[str] = None,
        description: Optional[str] = None,
        subtasks: Optional[List[Dict[str, Any]]] = None,
        result: Optional[Dict[str, Any]] = None,
        status: str = 'pending'
    ) -> Dict[str, Any]:
        """
        Cr√©er un message standardis√© pour les t√¢ches, sous-t√¢ches et synth√®se.
        
        Args:
            task_type (str): Type de t√¢che ('task', 'subtask', 'synthesis')
            request_id (str): ID unique de la demande
            sub_task_id (str, optional): ID de la sous-t√¢che
            original_request (str, optional): Requ√™te originale
            description (str, optional): Description de la t√¢che
            subtasks (List[Dict], optional): Liste des sous-t√¢ches
            result (Dict, optional): R√©sultat de la t√¢che
            status (str, optional): Statut de la t√¢che
        
        Returns:
            Dict[str, Any]: Message standardis√©
        """
        message = {
            'message_type': 'task_progress',
            'task_type': task_type,
            'request_id': request_id,
            'sub_task_id': sub_task_id,
            'status': status,
            'timestamp': datetime.now().isoformat(),
            'metadata': {
                'original_request': original_request,
                'description': description
            }
        }
        
        # Ajouter des informations sp√©cifiques selon le type de t√¢che
        if subtasks:
            message['subtasks'] = [
                {
                    'sub_task_id': subtask.get('sub_task_id', str(uuid.uuid4())),
                    'description': subtask.get('description'),
                    'status': subtask.get('status', 'pending')
                } 
                for subtask in subtasks
            ]
            message['total_subtasks'] = len(subtasks)
        
        if result:
            message['result'] = result
        
        return message

    async def decompose_task(self, user_request: str) -> TaskLedger:
        """
        D√©composer la requ√™te utilisateur en sous-t√¢ches avec function calling
        
        Args:
            user_request (str): La requ√™te utilisateur originale
        
        Returns:
            TaskLedger: Le registre de t√¢ches mis √† jour
        """
        try:
            # V√©rification pr√©liminaire de d√©composition
            needs_decomposition = self.should_decompose_task(user_request)

            # Si pas besoin de d√©composition, traitement simple
            if not needs_decomposition:
                # Ajouter directement la t√¢che au registre
                self.task_ledger.add_task(user_request)
                return self.task_ledger, needs_decomposition

            # D√©composer la t√¢che
            logger.info(f"üß© D√©composition de la t√¢che principale : {user_request}")
            detailed_subtasks = self._generate_detailed_subtasks(user_request)
            
            # Ajouter des sub_task_id aux sous-t√¢ches
            for subtask in detailed_subtasks:
                subtask['sub_task_id'] = str(uuid.uuid4())
            
            # Mettre √† jour le TaskLedger
            self.task_ledger.current_plan = [
                subtask['description']
                for subtask in detailed_subtasks
            ]
            
            # Log d√©taill√© des sous-t√¢ches identifi√©es
            logger.info(f"üìã Nombre de sous-t√¢ches identifi√©es : {len(self.task_ledger.current_plan)}")
            for idx, subtask in enumerate(self.task_ledger.current_plan, 1):
                logger.info(f"üî¢ Sous-t√¢che {idx}: {subtask}")
            
            # Pr√©parer le message de t√¢che principal
            task_message = self._create_task_message(
                task_type='task',
                request_id=self.task_ledger.task_id,
                original_request=user_request,
                subtasks=detailed_subtasks,
                status='started'
            )
            
            # Publier le message dans la queue de progression
            self._publish_rabbitmq_message('queue_progress_task', task_message)
            
            return self.task_ledger, needs_decomposition
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la d√©composition de t√¢che : {e}")
            logger.error(f"üîç Trace compl√®te : {traceback.format_exc()}")
            
            # En cas d'erreur, retourner le TaskLedger avec la t√¢che originale
            self.task_ledger.current_plan = [user_request]
            return self.task_ledger

    async def execute_task(
        self, 
        task_ledger: TaskLedger, 
        needs_decomposition: bool,
        dev_mode: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Ex√©cuter les sous-t√¢ches de mani√®re unifi√©e
        
        Args:
            task_ledger (TaskLedger): Le registre de t√¢ches √† ex√©cuter
            dev_mode (bool): Mode d√©veloppement qui simule l'ex√©cution
        
        Returns:
            List[Dict[str, Any]]: R√©sultats des sous-t√¢ches
        """
        subtask_results = []
        
        try:
            # Cas sans d√©composition : traitement direct
            if not needs_decomposition:
                logger.info("üöÄ Ex√©cution directe de la t√¢che sans d√©composition")
                
                # S√©lectionner le meilleur agent
                selected_agent = self._select_best_agent(task_ledger.original_request)
                logger.info(f"ü§ñ Agent s√©lectionn√© : {selected_agent.name}")
                
                try:
                    # Ex√©cution directe de la t√¢che
                    start_time = time.time()
                    
                    if hasattr(selected_agent, 'run'):
                        logger.debug("üì° Utilisation de la m√©thode synchrone run()")
                        result = selected_agent.run(task_ledger.original_request)
                    
                    elif hasattr(selected_agent, 'arun'):
                        logger.debug("üì° Utilisation de la m√©thode asynchrone arun()")
                        result = await selected_agent.arun(task_ledger.original_request)
                    
                    else:
                        logger.warning("‚ö†Ô∏è Aucune m√©thode run() ou arun() trouv√©e, utilisation du mod√®le LLM direct")
                        result = selected_agent.model(task_ledger.original_request)
                    
                    end_time = time.time()
                    execution_time = end_time - start_time
                    
                    logger.info(f"‚ú® R√©sultat de la t√¢che : {result.content[:200]}...")
                    logger.info(f"‚è±Ô∏è Temps d'ex√©cution : {execution_time:.2f} secondes")
                    
                    # Formater le r√©sultat comme une liste de dictionnaires pour √™tre coh√©rent
                    return [{
                        'task': task_ledger.original_request,
                        'result': result.content,
                        'agent': selected_agent.name,
                        'execution_time': execution_time
                    }]
                
                except Exception as e:
                    logger.error(f"‚ùå Erreur lors de l'ex√©cution directe : {e}")
                    logger.debug(f"üîç Trace compl√®te : {traceback.format_exc()}")
                    return []

            # Cas avec d√©composition (code existant)
            subtask_results = []
            
            # Parcourir les sous-t√¢ches du registre
            for task_index, task in enumerate(task_ledger.current_plan, 1):
                logger.info(f"üî¢ Sous-t√¢che {task_index}/{len(task_ledger.current_plan)}: {task}")
                
                # S√©lectionner dynamiquement l'agent
                selected_agent = self._select_best_agent(task)
                
                # G√©n√©rer un ID unique pour cette sous-t√¢che
                sub_task_id = str(uuid.uuid4())
                
                # Ex√©cuter la sous-t√¢che avec v√©rification des m√©thodes disponibles de l'agent
                logger.info(f"üîç Ex√©cution de la t√¢che : {task}")
                
                # R√©cup√©rer le nom de l'agent de plusieurs mani√®res
                agent_name = (
                    getattr(selected_agent, 'name', None) or  # Attribut 'name' d√©fini lors de la cr√©ation
                    getattr(selected_agent, '__name__', None) or  # Nom de la classe
                    selected_agent.__class__.__name__  # Nom de la classe par d√©faut
                )
                logger.info(f"ü§ñ R√©alisation de la sous-t√¢che par : {agent_name}")

                try:
                    start_time = time.time()
                    
                    if hasattr(selected_agent, 'run'):
                        logger.debug("üì° Utilisation de la m√©thode synchrone run()")
                        result = selected_agent.run(task)
                        logger.debug(f"‚úÖ M√©thode run() ex√©cut√©e avec succ√®s pour {selected_agent.__class__.__name__}")
                    
                    elif hasattr(selected_agent, 'arun'):
                        logger.debug("üì° Utilisation de la m√©thode asynchrone arun()")
                        result = await selected_agent.arun(task)
                        logger.debug(f"‚úÖ M√©thode arun() ex√©cut√©e avec succ√®s pour {selected_agent.__class__.__name__}")
                    
                    else:
                        logger.warning("‚ö†Ô∏è Aucune m√©thode run() ou arun() trouv√©e, utilisation du mod√®le LLM direct")
                        result = selected_agent.model(task)
                        logger.debug(f"‚úÖ Mod√®le LLM utilis√© pour {selected_agent.__class__.__name__}")

                    end_time = time.time()
                    execution_time = end_time - start_time
                    
                    # Log d√©taill√© du r√©sultat de la sous-t√¢che
                    logger.info(f"‚ú® R√©sultat de la sous-t√¢che : {result.content[:200]}...")
                    logger.info(f"‚è±Ô∏è Temps d'ex√©cution : {execution_time:.2f} secondes")
                    

                except Exception as e:
                    logger.error(f"‚ùå Erreur lors de l'ex√©cution de l'agent {selected_agent.__class__.__name__}")
                    logger.error(f"üî¥ D√©tails de l'erreur : {str(e)}")
                    logger.error(f"üîç Trace compl√®te : {traceback.format_exc()}")
                    result = None
                
                # Pr√©parer le message de r√©sultat de sous-t√¢che
                subtask_result_message = self._create_task_message(
                    task_type='subtask',
                    request_id=task_ledger.task_id,
                    sub_task_id=sub_task_id,
                    original_request=task,
                    status='completed',
                    result={
                        "content": result.content if result else "Aucun r√©sultat",
                        "content_type": result.content_type if result else "error",
                        "agent": selected_agent.name
                    }
                )
                
                # Publier le message de r√©sultat de sous-t√¢che
                self._publish_rabbitmq_message('queue_progress_task', subtask_result_message)
                
                # Stocker le r√©sultat
                subtask_results.append({
                    'result': result.content if result else "Aucun r√©sultat",
                    'agent': selected_agent.name
                })
    
            return subtask_results
    
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'ex√©cution des t√¢ches : {e}")
            return []

    async def process_request(
        self, 
        user_request: str, 
        debug_mode: bool = False
    ) -> Dict[str, Any]:
        """
        Traiter une requ√™te de bout en bout
        
        Args:
            user_request (str): La requ√™te utilisateur
            debug_mode (bool): Mode de d√©bogage
        
        Returns:
            Dict[str, Any]: R√©sultats du traitement
        """
        try:
            # D√©composer la t√¢che
            task_ledger, needs_decomposition = await self.decompose_task(user_request)
            
            # Ex√©cuter les sous-t√¢ches
            subtask_results = await self.execute_task(task_ledger, needs_decomposition)
            
            # Synth√©tiser les r√©sultats
            synthesized_result = await self._synthesize_results(subtask_results)
            
            # Publier un message RabbitMQ avec la synth√®se
            synthesis_message = self._create_task_message(
                task_type='synthesis',
                request_id=task_ledger.task_id,
                original_request=user_request,
                status='completed',
                result={
                    "content": synthesized_result,
                    "content_type": "text/plain"
                }
            )
            
            # Publier le message de synth√®se
            self._publish_rabbitmq_message('queue_progress_task', synthesis_message)
            
            # Log d√©taill√©
            logger.info(f"üìä R√©sultat synth√©tis√© : {synthesized_result}")
            
            return {
                'query': user_request,
                'result': synthesized_result,
                'agent_used': 'Multi-Purpose Intelligence Team',
                'metadata': {},
                'error': None,
                'task_id': task_ledger.task_id
            }
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement de la requ√™te : {e}")
            return {
                'query': user_request,
                'result': '',
                'agent_used': 'Multi-Purpose Intelligence Team',
                'metadata': {},
                'error': str(e),
                'task_id': None
            }

    async def _synthesize_results(
        self, 
        subtask_results: List[Dict[str, Any]]
    ) -> str:
        """
        Synth√©tiser les r√©sultats de plusieurs sous-t√¢ches
        
        Args:
            subtask_results (List[Dict[str, Any]]): Liste des r√©sultats de sous-t√¢ches
        
        Returns:
            str: R√©sultat synth√©tis√©
        """
        logger.info("üèÅ D√©but de la synth√®se des r√©sultats")
        
        # Convertir les r√©sultats en format texte
        text_results = [
            result.get('result', '') 
            for result in subtask_results 
            if result.get('result')
        ]
        
        # Cas sp√©cial : r√©sultat unique
        if len(text_results) == 1:
            return text_results[0]
        
        # Cas o√π aucun r√©sultat n'est disponible
        if not text_results:
            return "Aucun r√©sultat n'a pu √™tre g√©n√©r√©."
        
        # Utiliser l'agent orchestrateur pour synth√©tiser
        synthesis_prompt = f"""
        Synth√©tise les r√©sultats suivants de mani√®re concise et claire :
        
        {chr(10).join(text_results)}
        
        R√®gles pour la synth√®se :
        - Si plusieurs √©tapes, num√©rote et r√©sume chaque √©tape
        - Fournis un r√©sum√© final qui capture l'essence de tous les r√©sultats
        - Sois concis mais informatif
        """
        
        # Utiliser le mod√®le pour g√©n√©rer la synth√®se
        try:
            synthesis_response = await self.agent.arun(synthesis_prompt)
            return synthesis_response.content
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la synth√®se : {e}")
            # Retourner une synth√®se par d√©faut en cas d'erreur
            return " | ".join(text_results)

    def _generate_detailed_subtasks(self, user_request: str) -> List[Dict[str, Any]]:
        """
        G√©n√©rer des sous-t√¢ches d√©taill√©es pour une requ√™te utilisateur
        
        Args:
            user_request (str): La requ√™te originale de l'utilisateur
        
        Returns:
            List[Dict[str, Any]]: Liste des sous-t√¢ches d√©taill√©es
        """
        try:
            # Pr√©parer le prompt pour la g√©n√©ration de sous-t√¢ches
            subtasks_prompt = """
            D√©compose la t√¢che suivante en sous-t√¢ches essentielles et non redondantes.
            
            T√¢che principale : {user_request}
            
            Instructions:
            1. Analyse la t√¢che en d√©tail
            2. Identifie les actions concr√®tes n√©cessaires
            3. √âvite les √©tapes redondantes de rapport de r√©sultat
            4. Concentre-toi sur les actions productives
            
            Format de r√©ponse REQUIS (JSON strict) :
            {{
                "subtasks": [
                    {{
                        "task_id": "identifiant_unique",
                        "description": "Description concise et pr√©cise de la sous-t√¢che",
                        "priority": "haute|moyenne|basse"                    
                    }}
                ]
            }}
            """.format(user_request=user_request)
            
            # G√©n√©rer les sous-t√¢ches directement avec le client OpenAI
            response = self._openai_client.chat.completions.create(
                model=self.model_id,
                messages=[
                    {"role": "system", "content": "Tu es un expert en d√©composition de t√¢ches complexes, privil√©giant la concision et l'efficacit√©."},
                    {"role": "user", "content": subtasks_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            
            # Extraire le contenu de la r√©ponse
            subtasks_response = response.choices[0].message.content
            
            # V√©rifier que la r√©ponse est une cha√Æne
            if not isinstance(subtasks_response, str):
                logger.error(f"La r√©ponse du mod√®le n'est pas une cha√Æne : {type(subtasks_response)}")
                raise ValueError("R√©ponse du mod√®le invalide")
            
            # Convertir la r√©ponse en liste de sous-t√¢ches
            try:
                subtasks_dict = json.loads(subtasks_response)
            except json.JSONDecodeError as json_err:
                logger.error(f"Erreur de d√©codage JSON : {json_err}")
                logger.error(f"R√©ponse re√ßue : {subtasks_response}")
                raise
            
            subtasks = subtasks_dict.get('subtasks', [])
            
            # V√©rifier que le format est correct
            if not isinstance(subtasks, list):
                raise ValueError("La r√©ponse n'est pas une liste de sous-t√¢ches")

            return subtasks
        
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des sous-t√¢ches : {e}")
            # Retourner une d√©composition par d√©faut si la g√©n√©ration √©choue
            return [
                {
                    "task_id": "task_1",
                    "description": f"Analyser et ex√©cuter : {user_request}",
                    "priority": "haute"
                }
            ]

async def process_user_request(
    user_request: str, 
    debug_mode: bool = False,
    # Ajout des param√®tres de m√©moire et stockage
    db_url: str = 'postgresql+psycopg2://p4t:o3CCgX7StraZqvRH5GqrOFLuzt5R6C@vps-af24e24d.vps.ovh.net:30030/myboun',
    memory_table_name: str = "agent_memory",
    storage_table_name: str = "agent_sessions",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    Traiter une requ√™te utilisateur de mani√®re asynchrone avec l'orchestrateur
    
    Args:
        user_request (str): La requ√™te de l'utilisateur
        debug_mode (bool): Mode de d√©bogage
        db_url (str): URL de connexion √† la base de donn√©es PostgreSQL
        memory_table_name (str): Nom de la table de m√©moire
        storage_table_name (str): Nom de la table de stockage
        user_id (Optional[str]): Identifiant unique de l'utilisateur
        session_id (Optional[str]): Identifiant de session pour le suivi
    
    Returns:
        Dict[str, Any]: R√©sultat du traitement de la requ√™te
    """
    try:
        orchestrator = OrchestratorAgent(
            debug_mode=debug_mode, 
            original_request=user_request,
            # Ajout des param√®tres de m√©moire et stockage
            db_url=db_url,
            memory_table_name=memory_table_name,
            storage_table_name=storage_table_name,
            user_id=user_id,
            session_id=session_id,
            **kwargs
        )
        result = await orchestrator.process_request(
            user_request=user_request,
            debug_mode=debug_mode
        )
        
        # Extraction de la synth√®se
        synthesized_result = result.get('result', '')
        
        # D√©termination de l'agent utilis√©
        agent_used = 'Multi-Purpose Intelligence Team'
        task_results = result.get('task_results', {})
        
        # Cas sp√©cial : r√©sultat unique
        if len(task_results) > 1:
            agent_used = list(task_results.values())[0].get('agent', agent_used)
        
        # Cas o√π aucun r√©sultat n'est disponible
        elif len(task_results) == 1:
            first_task = list(task_results.keys())[0]
            task_result = task_results[first_task]
            
            # Si c'est un RunResponse
            if hasattr(task_result, 'content'):
                synthesized_result = task_result.content
                agent_used = task_result.name if hasattr(task_result, 'name') else agent_used
            
            # Si c'est un dictionnaire
            elif isinstance(task_result, dict):
                # Essayer d'extraire le contenu de diff√©rentes mani√®res
                if 'result' in task_result:
                    result_content = task_result['result']
                    if isinstance(result_content, dict):
                        synthesized_result = result_content.get('content', str(result_content))
                    elif isinstance(result_content, str):
                        synthesized_result = result_content
                
                agent_used = task_result.get('agent', agent_used)
        
        return {
            'query': user_request,
            'result': synthesized_result,
            'agent_used': agent_used,
            'metadata': {},
            'error': None
        }
    except Exception as e:
        logger.error(f"Erreur lors du traitement de la requ√™te : {e}")
        logger.error(traceback.format_exc())
        return {
            'query': user_request,
            'result': '',
            'agent_used': 'Multi-Purpose Intelligence Team',
            'metadata': {},
            'error': str(e)
        }

def get_orchestrator_agent(
    model_id: str = "gpt-4o-mini",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    debug_mode: bool = False,
    enable_web_agent: bool = True,
    enable_api_knowledge_agent: bool = False,
    enable_data_analysis_agent: bool = False,
    enable_travel_planner: bool = False,
    **kwargs
) -> OrchestratorAgent:
    """
    Cr√©er un agent orchestrateur avec configuration personnalisable
    
    Args:
        model_id (str): Identifiant du mod√®le OpenAI
        user_id (Optional[str]): Identifiant unique de l'utilisateur
        session_id (Optional[str]): Identifiant de session pour le suivi
        debug_mode (bool): Mode de d√©bogage
        enable_web_agent (bool): Activer l'agent de recherche web
        enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
        enable_data_analysis_agent (bool): Activer l'agent d'analyse de donn√©es
        enable_travel_planner (bool): Activer l'agent de planification de voyage
    
    Returns:
        OrchestratorAgent: Agent orchestrateur configur√©
    """
    # D√©finir les instructions de base
    instructions = [
        "ü§ñ Agent d'Orchestration Intelligent pour la R√©solution Adaptative des Requ√™tes",
        "Objectif Principal : Traiter Efficacement Chaque Requ√™te avec une Approche Multi-Strat√©gique",
        "",
        "üîç Strat√©gies de Traitement :",
        "1. Analyse Contextuelle Approfondie :",
        "   - √âvaluer la requ√™te initiale",
        "   - Consulter la m√©moire utilisateur pour enrichissement",
        "   - D√©terminer le mode de traitement optimal",
        "",
        "2. Modes de R√©solution :",
        "   a) Traitement Direct :",
        "      - R√©soudre imm√©diatement si la requ√™te est simple",
        "      - Utiliser les connaissances existantes",
        "   b) Enrichissement Contextuel :",
        "      - Consulter et int√©grer les informations de la m√©moire utilisateur",
        "      - Affiner et compl√©ter la requ√™te initiale",
        "   c) D√©composition Strat√©gique :",
        "      - Fragmenter les t√¢ches complexes en sous-t√¢ches pr√©cises",
        "      - Attribuer chaque sous-t√¢che √† l'agent sp√©cialis√©",
        "",
        "3. Routage Intelligent :",
        "   - S√©lectionner dynamiquement l'agent le plus adapt√© :",
        "     * Analyse s√©mantique de la requ√™te",
        "     * Correspondance avec les comp√©tences des agents disponibles",
        "     * √âvaluation de la complexit√© et du contexte",
        "     * Capacit√© √† cr√©er ou adapter des agents dynamiquement",
        "",
        "üß† Principes Op√©rationnels :",
        "- √ätre le point d'entr√©e unique et adaptatif pour toutes les requ√™tes",
        "- Maximiser la pr√©cision et la pertinence de la r√©ponse",
        "- Maintenir une tra√ßabilit√© compl√®te du processus de r√©solution",
        "- S'adapter dynamiquement aux diff√©rents types et complexit√©s de demandes"
    ]

    # Initialisation du mod√®le
    llm = OpenAIChat(
        model=model_id,
        temperature=0.2
    )

    # Cr√©er l'agent orchestrateur
    orchestrator_agent = OrchestratorAgent(
        model_id=model_id,
        instructions=instructions,
        model=llm,
        user_id=user_id,
        session_id=session_id,
        debug_mode=debug_mode,
        enable_web_agent=enable_web_agent,
        enable_api_knowledge_agent=enable_api_knowledge_agent,
        enable_data_analysis_agent=enable_data_analysis_agent,
        enable_travel_planner=enable_travel_planner,
        **kwargs
    )

    return orchestrator_agent
