import os
import sys
import asyncio
import logging
import traceback
import uuid
import json
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union, Callable
from dataclasses import dataclass, field
import time

import openai
from phi.agent import RunResponse, Agent, AgentMemory
from phi.model.openai import OpenAIChat
from phi.memory.db.postgres import PgMemoryDb
from phi.storage.agent.postgres import PgAgentStorage
from phi.storage.agent.sqlite import SqlAgentStorage
from phi.memory.db.sqlite import SqliteMemoryDb

from agents.web import get_web_searcher
from agents.settings import agent_settings
from agents.orchestrator_prompts import (
    get_task_decomposition_prompt,
    get_task_execution_prompt,
    get_task_synthesis_prompt
)

from utils.colored_logging import get_colored_logger



agent_memory_file: str = "orchestrator_agent_memory.db"
agent_storage_file: str = "orchestrator_agent_sessions.db"


# Configuration du logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Ajout d'un handler de console si nÃ©cessaire
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# Ajouter le rÃ©pertoire parent au PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

# Ajouter le handler au logger s'il n'est pas dÃ©jÃ  prÃ©sent
if not logger.handlers:
    logger.addHandler(console_handler)

AGENT_ROUTING_PROMPT = """
Pour la tÃ¢che : '{task}'
Et les agents suivants :
{agents_description}

Historique des performances :
{performance_history}

Quel agent est le plus appropriÃ© pour cette tÃ¢che ?
Fournir le nom de l'agent, le score de confiance et le raisonnement.
Format : JSON avec les clÃ©s 'selected_agent', 'confidence_score' et 'reasoning'
"""

TASK_CONTEXT_PROMPT = """
Pour la tÃ¢che : '{task}'
Et le contexte suivant :
{context}

Analyser le contexte et fournir les Ã©lÃ©ments suivants :
- Une analyse du contexte
- Une approche recommandÃ©e
- Des considÃ©rations critiques
Format : JSON avec les clÃ©s 'context_analysis', 'recommended_approach' et 'critical_considerations'
"""

@dataclass
class TaskLedger:
    """
    Registre pour gÃ©rer les faits et le plan de tÃ¢ches
    """
    original_request: str
    task_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    initial_plan: List[str] = field(default_factory=list)
    _current_plan: List[str] = field(default_factory=list, repr=False)
    facts: Dict[str, Any] = field(default_factory=lambda: {
        "verified": [],  # Faits vÃ©rifiÃ©s
        "to_lookup": [], # Faits Ã  rechercher
        "derived": [],   # Faits dÃ©rivÃ©s (calculs/logique)
        "guesses": []    # Suppositions Ã©duquÃ©es
    })
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    context: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not isinstance(self._current_plan, list):
            if isinstance(self._current_plan, dict):
                self._current_plan = list(self._current_plan.values())
            elif isinstance(self._current_plan, str):
                self._current_plan = [self._current_plan]
            else:
                self._current_plan = [str(self._current_plan)] if self._current_plan is not None else []

    @property
    def current_plan(self) -> List[str]:
        return self._current_plan

    @current_plan.setter
    def current_plan(self, value: Union[List[str], Dict[str, Any], str, Any]):
        if not isinstance(value, list):
            if isinstance(value, dict):
                value = list(value.values())
            elif isinstance(value, str):
                value = [value]
            else:
                value = [str(value)] if value is not None else []
        self._current_plan = value
        self.updated_at = datetime.now()

    def add_fact(self, fact: str, fact_type: str = "verified"):
        """Ajouter un fait au registre"""
        if fact_type in self.facts:
            if fact not in self.facts[fact_type]:
                self.facts[fact_type].append(fact)
                self.updated_at = datetime.now()

    def add_task(self, task: str):
        """Ajouter une tÃ¢che au plan courant"""
        if task not in self._current_plan:
            self._current_plan.append(task)
            self.updated_at = datetime.now()

    def register_agent(self, agent: Agent, agent_name: str):
        """Enregistrer un agent dans le registre"""
        self.context[agent_name] = agent

    def to_json(self) -> Dict[str, Any]:
        """
        Convertir le TaskLedger en format JSON
        """
        return {
            "task_id": self.task_id,
            "original_request": self.original_request,
            "initial_plan": self.initial_plan,
            "current_plan": self.current_plan,
            "facts": self.facts,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "context": self.context
        }

@dataclass
class ProgressLedger:
    """
    Registre pour suivre la progression et gÃ©rer les blocages
    """
    task_ledger: TaskLedger
    completed_tasks: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    task_history: List[Dict[str, Any]] = field(default_factory=list)
    agent_performance: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    stall_count: int = 0
    max_stalls: int = 2
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    status: str = "pending"

    def is_task_complete(self) -> bool:
        """VÃ©rifie si toutes les tÃ¢ches sont terminÃ©es"""
        return len(self.task_ledger.current_plan) == 0

    def is_making_progress(self) -> bool:
        """VÃ©rifie si des progrÃ¨s sont rÃ©alisÃ©s"""
        if not self.task_history:
            return True
        last_update = datetime.fromisoformat(self.task_history[-1]["completed_at"])
        time_since_last_update = datetime.now() - last_update
        return time_since_last_update.seconds < 300  # 5 minutes

    def is_stalled(self) -> bool:
        """VÃ©rifie si l'exÃ©cution est bloquÃ©e"""
        return self.stall_count >= self.max_stalls

    def complete_task(self, task: str, result: Dict[str, Any]):
        """Marquer une tÃ¢che comme terminÃ©e"""
        agent_name = result.get('agent', 'unknown')
        
        # Mettre Ã  jour les performances de l'agent
        if agent_name not in self.agent_performance:
            self.agent_performance[agent_name] = {
                'tasks_completed': 0,
                'success_rate': 1.0,
                'total_tasks': 0,
                'recent_performance': []
            }
        
        performance = self.agent_performance[agent_name]
        performance['total_tasks'] += 1
        
        if 'error' not in result:
            performance['tasks_completed'] += 1
            performance['success_rate'] = performance['tasks_completed'] / performance['total_tasks']
            performance['recent_performance'].append('success')
        else:
            performance['recent_performance'].append('failure')
        
        # Stocker le rÃ©sultat
        self.completed_tasks[task] = result
        self.task_history.append({
            "task": task,
            "result": result,
            "completed_at": datetime.now().isoformat()
        })
        
        # Retirer la tÃ¢che du plan courant
        if task in self.task_ledger.current_plan:
            self.task_ledger.current_plan.remove(task)
        
        self.updated_at = datetime.now()

    def increment_stall(self):
        """IncrÃ©menter le compteur de blocages"""
        self.stall_count += 1
        self.updated_at = datetime.now()

    def reset_stall(self):
        """RÃ©initialiser le compteur de blocages"""
        self.stall_count = 0
        self.updated_at = datetime.now()

    def get_next_agent(self, task: str) -> str:
        """
        SÃ©lectionner le prochain agent basÃ© sur les performances
        """
        best_agent = None
        best_score = -1

        for agent_name, perf in self.agent_performance.items():
            score = perf['success_rate'] * (perf['tasks_completed'] + 1)
            if score > best_score:
                best_score = score
                best_agent = agent_name

        return best_agent or "API Knowledge Agent"  # Agent par dÃ©faut

    def to_json(self) -> Dict[str, Any]:
        """Convertir en format JSON"""
        def convert_result(result):
            if hasattr(result, 'content'):
                return {
                    "content": result.content,
                    "content_type": str(result.content_type),
                    "event": str(result.event)
                }
            return result

        return {
            "task_ledger": self.task_ledger.to_json(),
            "completed_tasks": {
                task: {k: convert_result(v) for k, v in result.items()} 
                for task, result in self.completed_tasks.items()
            },
            "task_history": [
                {
                    "task": entry["task"],
                    "result": convert_result(entry["result"]),
                    "completed_at": entry["completed_at"]
                } 
                for entry in self.task_history
            ],
            "agent_performance": self.agent_performance,
            "stall_count": self.stall_count,
            "status": self.status
        }

class OrchestratorAgent(Agent):
    """
    Agent orchestrateur avancÃ© avec dÃ©composition de tÃ¢ches
    """
    def __init__(
        self,
        model_id: str = "gpt-4o-mini", 
        debug_mode: bool = False,
        original_request: Optional[str] = None,
        api_key: Optional[str] = None,  
        enable_web_agent: bool = True,
        enable_api_knowledge_agent: bool = False,
        enable_data_analysis_agent: bool = False,
        enable_travel_planner: bool = False,
        # Ajout des paramÃ¨tres de la classe Agent
        name: str = "Orchestrator Agent",
        instructions: Optional[List[str]] = None,
        tools: Optional[List[Callable]] = None,
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
        storage: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        Initialiser l'agent orchestrateur avec des agents spÃ©cialisÃ©s et une mÃ©moire persistante
        
        Args:
            model_id (str): Identifiant du modÃ¨le OpenAI
            debug_mode (bool): Mode de dÃ©bogage
            original_request (Optional[str]): RequÃªte originale pour le TaskLedger
            api_key (Optional[str]): ClÃ© API OpenAI personnalisÃ©e
            enable_web_agent (bool): Activer l'agent de recherche web
            enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
            enable_data_analysis_agent (bool): Activer l'agent d'analyse de donnÃ©es
            enable_travel_planner (bool): Activer l'agent de planification de voyage
            db_url (str): URL de connexion Ã  la base de donnÃ©es PostgreSQL
            memory_table_name (str): Nom de la table de mÃ©moire
            storage_table_name (str): Nom de la table de stockage
        """
        # PrÃ©parer les instructions par dÃ©faut si non fournies
        default_instructions = [
            "Tu es un agent d'orchestration intelligent.",
            "DÃ©compose les tÃ¢ches complexes en sous-tÃ¢ches gÃ©rables.",
            "SÃ©lectionne et coordonne les agents spÃ©cialisÃ©s.",
            "Assure une exÃ©cution efficace et cohÃ©rente des tÃ¢ches."
        ]
        instructions = kwargs.get('instructions', default_instructions)
        # Configuration du stockage
        storage = SqlAgentStorage(
            table_name="agent_sessions", 
            db_file=agent_storage_file
        )

        # PrÃ©paration des paramÃ¨tres pour l'initialisation
        agent_init_kwargs = {
            "name": "Orchestrator Agent",
            "instructions": instructions,
            "tools": tools,
            "user_id": user_id,
            "session_id": session_id,
            "memory": AgentMemory(
                # db=PgMemoryDb(
                #     table_name=kwargs.get('memory_table_name', 'orchestrator_agent_memory'), 
                #     db_url=kwargs.get('db_url', 'postgresql+psycopg2://p4t:o3CCgX7StraZqvRH5GqrOFLuzt5R6C@vps-af24e24d.vps.ovh.net:30030/myboun')
                # ),
                    db=SqliteMemoryDb(
                        table_name="agent_memory",
                        db_file=agent_memory_file,
                    ),
            # Create and store personalized memories for this user
            create_user_memories=True,
            # Update memories for the user after each run
            update_user_memories_after_run=True,
            # Create and store session summaries
            create_session_summary=True,
            # Update session summaries after each run
            update_session_summary_after_run=True,
            ),
            "storage": storage,
            "add_history_to_messages": True,
            "num_history_responses": 3
        }
        
        # Ajouter les kwargs supplÃ©mentaires
        agent_init_kwargs.update(kwargs)

        # Appel du constructeur parent de Agent
        super().__init__(
            name="Orchestrator Agent",
            instructions=instructions,
            tools=tools,
            user_id=user_id,
            session_id=session_id,
            memory=memory,
            storage=storage,
            **kwargs
        )
        
        # Configuration du modÃ¨le LLM
        self.llm_config = {
            "model": model_id,
            "temperature": 0.2
        }
        
        # Utiliser la clÃ© API fournie ou celle de l'environnement
        openai_api_key = api_key or os.getenv('OPENAI_API_KEY')
        
        # Initialisation explicite du client OpenAI
        try:
            self.client = openai.OpenAI(api_key=openai_api_key)
        except Exception as e:
            logger.error(f"âŒ Erreur d'initialisation du client OpenAI : {e}")
            self.client = None
        
        # Initialiser le modÃ¨le OpenAI
        self.model = self.client.chat.completions.create
        
        # Initialiser self.llm comme alias de self.model
        self.llm = self.model
        
        # Initialisation centralisÃ©e des agents
        self.agents = self._initialize_specialized_agents(
            enable_web_agent=enable_web_agent,
            enable_api_knowledge_agent=enable_api_knowledge_agent,
            enable_data_analysis_agent=enable_data_analysis_agent,
            enable_travel_planner=enable_travel_planner
        )
        
        # Initialisation du mode de dÃ©bogage
        self.debug_mode = debug_mode
        
        # CrÃ©ation du TaskLedger initial
        self.task_ledger = self._create_task_ledger(original_request)
        
        # CrÃ©er l'agent orchestrateur avec configuration simplifiÃ©e
        self.agent = self._create_orchestrator_agent(debug_mode)

    def run(self, task: str) -> RunResponse:
        """
        MÃ©thode run standard pour l'Agent Phidata
        DÃ©lÃ¨gue au processus de traitement de requÃªte existant
        """
        try:
            result = self.process_request(task, debug_mode=self.debug_mode)
            return RunResponse(
                content=result.get('final_result', 'Aucun rÃ©sultat'),
                content_type='text',
                metadata=result
            )
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'exÃ©cution de l'orchestrateur : {e}")
            return RunResponse(
                content=f"Erreur : {str(e)}",
                content_type='error'
            )

    def _initialize_specialized_agents(
        self, 
        enable_web_agent: bool = True,
        enable_api_knowledge_agent: bool = False,
        enable_data_analysis_agent: bool = False,
        enable_travel_planner: bool = False
    ) -> Dict[str, Any]:
        """
        Initialiser les agents spÃ©cialisÃ©s
        
        Args:
            enable_web_agent (bool): Activer l'agent de recherche web
            enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
            enable_data_analysis_agent (bool): Activer l'agent d'analyse de donnÃ©es
            enable_travel_planner (bool): Activer l'agent de planification de voyage
        
        Returns:
            Dict[str, Any]: Dictionnaire des agents initialisÃ©s
        """
        logger.info("ğŸ¤– Initialisation des agents spÃ©cialisÃ©s")
        logger.info(f"ğŸŒ Web Agent: {enable_web_agent}")
        logger.info(f"ğŸ“š API Knowledge Agent: {enable_api_knowledge_agent}")
        logger.info(f"ğŸ“Š Data Analysis Agent: {enable_data_analysis_agent}")
        logger.info(f"âœˆï¸ Travel Planner Agent: {enable_travel_planner}")
        
        agents = {}
        
        # Initialisation de l'agent de recherche web
        if enable_web_agent:
            try:
                web_agent = get_web_searcher()
                agents['web_agent'] = web_agent
                logger.info("âœ… Agent de recherche web initialisÃ© avec succÃ¨s")
            except Exception as e:
                logger.error(f"âŒ Erreur d'initialisation de l'agent de recherche web : {e}")
                logger.debug(traceback.format_exc())
        
        # Placeholder pour les autres agents (Ã  implÃ©menter si nÃ©cessaire)
        if enable_api_knowledge_agent:
            logger.warning("ğŸš§ API Knowledge Agent non implÃ©mentÃ©")
        
        if enable_data_analysis_agent:
            logger.warning("ğŸš§ Data Analysis Agent non implÃ©mentÃ©")
        
        if enable_travel_planner:
            logger.warning("ğŸš§ Travel Planner Agent non implÃ©mentÃ©")
        
        return agents

    def _create_task_ledger(self, original_request: Optional[str] = None) -> TaskLedger:
        """
        CrÃ©er un TaskLedger avec gestion robuste
        
        Args:
            original_request (Optional[str]): RequÃªte originale
        
        Returns:
            TaskLedger: Registre de tÃ¢ches initialisÃ©
        """
        return TaskLedger(
            original_request=original_request or "RequÃªte non spÃ©cifiÃ©e",
            context={name: agent for name, agent in self.agents.items()}
        )

    def _create_orchestrator_agent(self, debug_mode: bool = False) -> Agent:
        """
        CrÃ©er l'agent orchestrateur avec configuration simplifiÃ©e
        
        Args:
            debug_mode (bool): Mode de dÃ©bogage
        
        Returns:
            Agent: Agent orchestrateur configurÃ©
        """
        return Agent(
            # Utiliser la mÃ©thode create du client OpenAI
            llm=self.client.chat.completions.create,
            instructions=[
                "Tu es un agent d'orchestration avancÃ© capable de dÃ©composer des tÃ¢ches complexes.",
                "Ã‰tapes de travail :",
                "1. Analyser la requÃªte originale",
                "2. DÃ©composer la requÃªte en sous-tÃ¢ches prÃ©cises et rÃ©alisables",
                "3. Attribuer chaque sous-tÃ¢che Ã  l'agent le plus appropriÃ©",
                "4. Suivre la progression de chaque sous-tÃ¢che",
                "5. IntÃ©grer et synthÃ©tiser les rÃ©sultats partiels",
                "6. Adapter dynamiquement le plan si nÃ©cessaire"
            ],
            # Ajouter le mode de dÃ©bogage si nÃ©cessaire
            debug_mode=debug_mode
        )

    def _get_task_decomposition_functions(self) -> List[Dict[str, Any]]:
        """
        DÃ©finir les fonctions d'appel pour la dÃ©composition de tÃ¢ches
        
        Returns:
            List[Dict[str, Any]]: Liste des dÃ©finitions de fonctions
        """
        return [
            {
                "name": "decompose_task",
                "description": "DÃ©composer une tÃ¢che complexe en sous-tÃ¢ches prÃ©cises et ordonnÃ©es",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "subtasks": {
                            "type": "array",
                            "description": "Liste des sous-tÃ¢ches",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "task": {
                                        "type": "string", 
                                        "description": "Description prÃ©cise de la sous-tÃ¢che"
                                    },
                                    "agent": {
                                        "type": "string", 
                                        "description": "Agent recommandÃ© pour la sous-tÃ¢che",
                                        "enum": [
                                            "Web Search Agent", 
                                            "API Knowledge Agent", 
                                            "Travel Planner Agent"
                                        ]
                                    },
                                    "priority": {
                                        "type": "string", 
                                        "description": "PrioritÃ© de la sous-tÃ¢che",
                                        "enum": ["haute", "moyenne", "basse"]
                                    }
                                },
                                "required": ["task", "agent", "priority"]
                            }
                        }
                    },
                    "required": ["subtasks"]
                }
            }
        ]

    def _get_agent_selection_functions(self) -> List[Dict[str, Any]]:
        """
        DÃ©finir les fonctions d'appel pour la sÃ©lection d'agents
        
        Args:
        
        Returns:
            List[Dict[str, Any]]: Liste des dÃ©finitions de fonctions
        """
        return [
            {
                "name": "select_best_agent",
                "description": "SÃ©lectionner l'agent le plus appropriÃ© pour une tÃ¢che donnÃ©e",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "task": {
                            "type": "string", 
                            "description": "Description de la tÃ¢che Ã  exÃ©cuter"
                        },
                        "selected_agent": {
                            "type": "object",
                            "description": "Agent sÃ©lectionnÃ© avec son score de confiance",
                            "properties": {
                                "name": {
                                    "type": "string", 
                                    "description": "Nom de l'agent",
                                    "enum": [
                                        "Web Search Agent", 
                                        #"API Knowledge Agent", 

                                    ]
                                },
                                "confidence_score": {
                                    "type": "number", 
                                    "description": "Score de confiance (0.0-1.0)",
                                    "minimum": 0,
                                    "maximum": 1
                                },
                                "reasoning": {
                                    "type": "string", 
                                    "description": "Justification de la sÃ©lection"
                                }
                            },
                            "required": ["name", "confidence_score", "reasoning"]
                        }
                    },
                    "required": ["task", "selected_agent"]
                }
            }
        ]

    def should_decompose_task(self, user_request: str) -> bool:
        """
        Utiliser le LLM pour dÃ©terminer si la tÃ¢che nÃ©cessite une dÃ©composition
        
        Args:
            user_request (str): La requÃªte utilisateur originale
        
        Returns:
            bool: True si dÃ©composition nÃ©cessaire, False sinon
        """
        try:
            response = self.client.chat.completions.create(
                model=self.llm_config["model"],
                messages=[
                    {
                        "role": "system", 
                        "content": """
                        Tu es un expert en analyse de tÃ¢ches complexes.
                        
                        CRITÃˆRES DE DÃ‰COMPOSITION :
                        - La tÃ¢che nÃ©cessite-t-elle vraiment d'Ãªtre divisÃ©e ?
                        - PrÃ©sente-t-elle plusieurs dimensions ou Ã©tapes ?
                        - Requiert-elle diffÃ©rentes compÃ©tences ou approches ?
                        
                        NE PAS DÃ‰COMPOSER pour :
                        - RequÃªtes simples et directes
                        - TÃ¢ches ne nÃ©cessitant qu'une seule action
                        - Demandes courtes et prÃ©cises
                        
                        DÃ‰COMPOSER pour :
                        - Projets multi-Ã©tapes
                        - TÃ¢ches nÃ©cessitant planification
                        - RequÃªtes complexes avec plusieurs objectifs
                        
                        RÃ©ponds par "OUI" ou "NON" avec discernement
                        """
                    },
                    {
                        "role": "user", 
                        "content": f"Analyse cette requÃªte : {user_request}"
                    }
                ],
                max_tokens=10,
                temperature=0.2
            )
            
            llm_decision = response.choices[0].message.content.strip().upper()
            logger.info(f"DÃ©cision de dÃ©composition pour '{user_request}': {llm_decision}")
            
            return llm_decision == "OUI"
        
        except Exception as e:
            logger.error(f"Erreur lors de la dÃ©cision de dÃ©composition : {e}")
            return False

    def _select_best_agent(self, task: str) -> Agent:
        """
        SÃ©lectionner dynamiquement le meilleur agent pour une tÃ¢che donnÃ©e
        
        Args:
            task (str): La tÃ¢che Ã  exÃ©cuter
        
        Returns:
            Agent: L'agent le plus appropriÃ©
        """
        # SÃ©lection par LLM
        try:
            # Utiliser l'API OpenAI pour classifier la tÃ¢che
            response = self.client.chat.completions.create(
                model=self.llm_config['model'],
                messages=[
                    {
                        "role": "system", 
                        "content": "Tu es un assistant qui aide Ã  sÃ©lectionner le bon agent parmi une liste."
                    },
                    {
                        "role": "user", 
                        "content": f"""
                        Analyse la tÃ¢che suivante et choisis l'agent le plus adaptÃ© en fonction de ses compÃ©tences :

                        TÃ¢che : {task}
                        
                        Liste des agents disponibles : {list(self.agents.keys())}

                        CritÃ¨res de sÃ©lection :
                        - CompatibilitÃ© des compÃ©tences de l'agent avec la tÃ¢che.
                        - SpÃ©cialisation et capacitÃ© d'exÃ©cution de l'agent.
                        - RapiditÃ© et efficacitÃ© de l'agent pour accomplir la tÃ¢che.
                        
                        RÃ©ponds uniquement avec le nom de l'agent le plus adaptÃ©.
                        """
                    }
                ],
                temperature=0.2,
                max_tokens=300
            )
            
            # Extraire et traiter la classification
            classification = response.choices[0].message.content.strip().lower()
            logger.debug(f"ğŸ§  Classification de la tÃ¢che : {classification}")
            logger.info(f"ğŸ” Agents disponibles : {list(self.agents.keys())}")

            # Convertir le nom en agent
            selected_agent_name = classification.lower().replace(' ', '_')
            selected_agent = self.agents.get(selected_agent_name)
            
            logger.info(f"ğŸ† Agent sÃ©lectionnÃ© : {selected_agent_name}")
            return selected_agent
        
        except Exception as e:
            logger.error(f"âŒ Erreur de sÃ©lection d'agent : {e}")
            return next(iter(self.agents.values()))

    def _publish_rabbitmq_message(self, queue_name: str, message: Dict[str, Any]) -> bool:
        """
        Publier un message dans une file RabbitMQ
        
        Args:
            queue_name (str): Nom de la file
            message (Dict[str, Any]): Message Ã  publier
        
        Returns:
            bool: True si la publication a rÃ©ussi, False sinon
        """
        try:
            # Importer pika de maniÃ¨re sÃ©curisÃ©e
            import importlib
            import os
            from dotenv import load_dotenv
            
            # Charger les variables d'environnement
            load_dotenv()
            
            # RÃ©cupÃ©rer les paramÃ¨tres de connexion RabbitMQ
            rabbitmq_host = os.getenv('RABBITMQ_HOST', 'localhost')
            rabbitmq_port = int(os.getenv('RABBITMQ_PORT', 5672))
            rabbitmq_user = os.getenv('RABBITMQ_USER', '')
            rabbitmq_password = os.getenv('RABBITMQ_PASSWORD', '')
            
            # Importer pika
            pika = importlib.import_module('pika')
            
            # Configurer les paramÃ¨tres de connexion
            credentials = pika.PlainCredentials(rabbitmq_user, rabbitmq_password) if rabbitmq_user else None
            connection_params = pika.ConnectionParameters(
                host=rabbitmq_host,
                port=rabbitmq_port,
                credentials=credentials,
                connection_attempts=2,
                retry_delay=1
            )
            
            # Ã‰tablir la connexion
            try:
                connection = pika.BlockingConnection(connection_params)
            except (pika.exceptions.AMQPConnectionError, ConnectionRefusedError) as conn_error:
                logger.warning(f"Impossible de se connecter Ã  RabbitMQ : {conn_error}")
                return False
            
            try:
                channel = connection.channel()
                
                # DÃ©clarer la queue si elle n'existe pas
                channel.queue_declare(queue=queue_name, durable=True)
                
                # Publier le message
                channel.basic_publish(
                    exchange='',
                    routing_key=queue_name,
                    body=json.dumps(message),
                    properties=pika.BasicProperties(
                        delivery_mode=2,  # Rendre le message persistant
                        content_type='application/json'
                    )
                )
                
                logger.info(f"Message RabbitMQ publiÃ© dans la file {queue_name} sur {rabbitmq_host}:{rabbitmq_port}")
                return True
            
            except Exception as publish_error:
                logger.error(f"Erreur lors de la publication dans RabbitMQ : {publish_error}")
                return False
            
            finally:
                connection.close()
        
        except ImportError:
            logger.warning("La bibliothÃ¨que pika n'est pas installÃ©e.")
            return False
        
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la publication RabbitMQ : {e}")
            return False

    def _create_task_message(
        self, 
        task_type: str, 
        request_id: str, 
        sub_task_id: Optional[str] = None,
        original_request: Optional[str] = None,
        description: Optional[str] = None,
        subtasks: Optional[List[Dict[str, Any]]] = None,
        result: Optional[Dict[str, Any]] = None,
        status: str = 'pending'
    ) -> Dict[str, Any]:
        """
        CrÃ©er un message standardisÃ© pour les tÃ¢ches, sous-tÃ¢ches et synthÃ¨se.
        
        Args:
            task_type (str): Type de tÃ¢che ('task', 'subtask', 'synthesis')
            request_id (str): ID unique de la demande
            sub_task_id (str, optional): ID de la sous-tÃ¢che
            original_request (str, optional): RequÃªte originale
            description (str, optional): Description de la tÃ¢che
            subtasks (List[Dict], optional): Liste des sous-tÃ¢ches
            result (Dict, optional): RÃ©sultat de la tÃ¢che
            status (str, optional): Statut de la tÃ¢che
        
        Returns:
            Dict[str, Any]: Message standardisÃ©
        """
        message = {
            'message_type': 'task_progress',
            'task_type': task_type,
            'request_id': request_id,
            'sub_task_id': sub_task_id,
            'status': status,
            'timestamp': datetime.now().isoformat(),
            'metadata': {
                'original_request': original_request,
                'description': description
            }
        }
        
        # Ajouter des informations spÃ©cifiques selon le type de tÃ¢che
        if subtasks:
            message['subtasks'] = [
                {
                    'sub_task_id': subtask.get('sub_task_id', str(uuid.uuid4())),
                    'description': subtask.get('description'),
                    'status': subtask.get('status', 'pending')
                } 
                for subtask in subtasks
            ]
            message['total_subtasks'] = len(subtasks)
        
        if result:
            message['result'] = result
        
        return message

    async def decompose_task(self, user_request: str) -> TaskLedger:
        """
        DÃ©composer la requÃªte utilisateur en sous-tÃ¢ches avec function calling
        
        Args:
            user_request (str): La requÃªte utilisateur originale
        
        Returns:
            TaskLedger: Le registre de tÃ¢ches mis Ã  jour
        """
        logger.info(f"ğŸ§© DÃ©composition de la tÃ¢che principale : {user_request}")
        
        try:
            # DÃ©composer la tÃ¢che
            detailed_subtasks = self._generate_detailed_subtasks(user_request)
            
            # Ajouter des sub_task_id aux sous-tÃ¢ches
            for subtask in detailed_subtasks:
                subtask['sub_task_id'] = str(uuid.uuid4())
            
            # Mettre Ã  jour le TaskLedger
            self.task_ledger.current_plan = [
                subtask['description']
                for subtask in detailed_subtasks
            ]
            
            # Log dÃ©taillÃ© des sous-tÃ¢ches identifiÃ©es
            logger.info(f"ğŸ“‹ Nombre de sous-tÃ¢ches identifiÃ©es : {len(self.task_ledger.current_plan)}")
            for idx, subtask in enumerate(self.task_ledger.current_plan, 1):
                logger.info(f"ğŸ”¢ Sous-tÃ¢che {idx}: {subtask}")
            
            # PrÃ©parer le message de tÃ¢che principal
            task_message = self._create_task_message(
                task_type='task',
                request_id=self.task_ledger.task_id,
                original_request=user_request,
                subtasks=detailed_subtasks,
                status='started'
            )
            
            # Publier le message dans la queue de progression
            self._publish_rabbitmq_message('queue_progress_task', task_message)
            
            return self.task_ledger
        
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la dÃ©composition de tÃ¢che : {e}")
            logger.error(f"ğŸ” Trace complÃ¨te : {traceback.format_exc()}")
            
            # En cas d'erreur, retourner le TaskLedger avec la tÃ¢che originale
            self.task_ledger.current_plan = [user_request]
            return self.task_ledger

    async def execute_task(
        self, 
        task_ledger: TaskLedger, 
        dev_mode: bool = False
    ) -> List[Dict[str, Any]]:
        """
        ExÃ©cuter les sous-tÃ¢ches de maniÃ¨re unifiÃ©e
        
        Args:
            task_ledger (TaskLedger): Le registre de tÃ¢ches Ã  exÃ©cuter
            dev_mode (bool): Mode dÃ©veloppement qui simule l'exÃ©cution
        
        Returns:
            List[Dict[str, Any]]: RÃ©sultats des sous-tÃ¢ches
        """
        subtask_results = []
        
        try:
            # Parcourir les sous-tÃ¢ches du registre
            for task_index, task in enumerate(task_ledger.current_plan, 1):
                logger.info(f"ğŸ”¢ Sous-tÃ¢che {task_index}/{len(task_ledger.current_plan)}: {task}")
                
                # SÃ©lectionner dynamiquement l'agent
                selected_agent = self._select_best_agent(task)
                
                # GÃ©nÃ©rer un ID unique pour cette sous-tÃ¢che
                sub_task_id = str(uuid.uuid4())
                
                # ExÃ©cuter la sous-tÃ¢che avec vÃ©rification des mÃ©thodes disponibles de l'agent
                logger.info(f"ğŸ” ExÃ©cution de la tÃ¢che : {task}")
                
                # RÃ©cupÃ©rer le nom de l'agent de plusieurs maniÃ¨res
                agent_name = (
                    getattr(selected_agent, 'name', None) or  # Attribut 'name' dÃ©fini lors de la crÃ©ation
                    getattr(selected_agent, '__name__', None) or  # Nom de la classe
                    selected_agent.__class__.__name__  # Nom de la classe par dÃ©faut
                )
                logger.info(f"ğŸ¤– RÃ©alisation de la sous-tÃ¢che par : {agent_name}")

                try:
                    start_time = time.time()
                    
                    if hasattr(selected_agent, 'run'):
                        logger.debug("ğŸ“¡ Utilisation de la mÃ©thode synchrone run()")
                        result = selected_agent.run(task)
                        logger.debug(f"âœ… MÃ©thode run() exÃ©cutÃ©e avec succÃ¨s pour {selected_agent.__class__.__name__}")
                    
                    elif hasattr(selected_agent, 'arun'):
                        logger.debug("ğŸ“¡ Utilisation de la mÃ©thode asynchrone arun()")
                        result = await selected_agent.arun(task)
                        logger.debug(f"âœ… MÃ©thode arun() exÃ©cutÃ©e avec succÃ¨s pour {selected_agent.__class__.__name__}")
                    
                    else:
                        logger.warning("âš ï¸ Aucune mÃ©thode run() ou arun() trouvÃ©e, utilisation du modÃ¨le LLM direct")
                        result = selected_agent.model(task)
                        logger.debug(f"âœ… ModÃ¨le LLM utilisÃ© pour {selected_agent.__class__.__name__}")

                    end_time = time.time()
                    execution_time = end_time - start_time
                    
                    # Log dÃ©taillÃ© du rÃ©sultat de la sous-tÃ¢che
                    logger.info(f"âœ¨ RÃ©sultat de la sous-tÃ¢che : {result.content[:200]}...")
                    logger.info(f"â±ï¸ Temps d'exÃ©cution : {execution_time:.2f} secondes")
                    

                except Exception as e:
                    logger.error(f"âŒ Erreur lors de l'exÃ©cution de l'agent {selected_agent.__class__.__name__}")
                    logger.error(f"ğŸ”´ DÃ©tails de l'erreur : {str(e)}")
                    logger.error(f"ğŸ” Trace complÃ¨te : {traceback.format_exc()}")
                    result = None
                
                # PrÃ©parer le message de rÃ©sultat de sous-tÃ¢che
                subtask_result_message = self._create_task_message(
                    task_type='subtask',
                    request_id=task_ledger.task_id,
                    sub_task_id=sub_task_id,
                    original_request=task,
                    status='completed',
                    result={
                        "content": result.content if result else "Aucun rÃ©sultat",
                        "content_type": result.content_type if result else "error",
                        "agent": selected_agent.name
                    }
                )
                
                # Publier le message de rÃ©sultat de sous-tÃ¢che
                self._publish_rabbitmq_message('queue_progress_task', subtask_result_message)
                
                # Stocker le rÃ©sultat
                subtask_results.append({
                    'result': result.content if result else "Aucun rÃ©sultat",
                    'agent': selected_agent.name
                })
    
            return subtask_results
    
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'exÃ©cution des tÃ¢ches : {e}")
            return []

    async def process_request(
        self, 
        user_request: str, 
        debug_mode: bool = False
    ) -> Dict[str, Any]:
        """
        Traiter une requÃªte de bout en bout
        
        Args:
            user_request (str): La requÃªte utilisateur
            debug_mode (bool): Mode de dÃ©bogage
        
        Returns:
            Dict[str, Any]: RÃ©sultats du traitement
        """
        try:
            # DÃ©composer la tÃ¢che
            task_ledger = await self.decompose_task(user_request)
            
            # ExÃ©cuter les sous-tÃ¢ches
            subtask_results = await self.execute_task(task_ledger)
            
            # SynthÃ©tiser les rÃ©sultats
            synthesized_result = await self._synthesize_results(subtask_results)
            
            # Publier un message RabbitMQ avec la synthÃ¨se
            synthesis_message = self._create_task_message(
                task_type='synthesis',
                request_id=task_ledger.task_id,
                original_request=user_request,
                status='completed',
                result={
                    "content": synthesized_result,
                    "content_type": "text/plain"
                }
            )
            
            # Publier le message de synthÃ¨se
            self._publish_rabbitmq_message('queue_progress_task', synthesis_message)
            
            # Log dÃ©taillÃ©
            logger.info(f"ğŸ“Š RÃ©sultat synthÃ©tisÃ© : {synthesized_result}")
            
            return {
                'query': user_request,
                'result': synthesized_result,
                'agent_used': 'Multi-Purpose Intelligence Team',
                'metadata': {},
                'error': None,
                'task_id': task_ledger.task_id
            }
        
        except Exception as e:
            logger.error(f"âŒ Erreur lors du traitement de la requÃªte : {e}")
            return {
                'query': user_request,
                'result': '',
                'agent_used': 'Multi-Purpose Intelligence Team',
                'metadata': {},
                'error': str(e),
                'task_id': None
            }

    async def _synthesize_results(
        self, 
        subtask_results: List[Dict[str, Any]]
    ) -> str:
        """
        SynthÃ©tiser les rÃ©sultats de plusieurs sous-tÃ¢ches
        
        Args:
            subtask_results (List[Dict[str, Any]]): Liste des rÃ©sultats de sous-tÃ¢ches
        
        Returns:
            str: RÃ©sultat synthÃ©tisÃ©
        """
        logger.info("ğŸ DÃ©but de la synthÃ¨se des rÃ©sultats")
        
        # Convertir les rÃ©sultats en format texte
        text_results = [
            result.get('result', '') 
            for result in subtask_results 
            if result.get('result')
        ]
        
        # Cas spÃ©cial : rÃ©sultat unique
        if len(text_results) == 1:
            return text_results[0]
        
        # Cas oÃ¹ aucun rÃ©sultat n'est disponible
        if not text_results:
            return "Aucun rÃ©sultat n'a pu Ãªtre gÃ©nÃ©rÃ©."
        
        # Utiliser l'agent orchestrateur pour synthÃ©tiser
        synthesis_prompt = f"""
        SynthÃ©tise les rÃ©sultats suivants de maniÃ¨re concise et claire :
        
        {chr(10).join(text_results)}
        
        RÃ¨gles pour la synthÃ¨se :
        - Si plusieurs Ã©tapes, numÃ©rote et rÃ©sume chaque Ã©tape
        - Fournis un rÃ©sumÃ© final qui capture l'essence de tous les rÃ©sultats
        - Sois concis mais informatif
        """
        
        # Utiliser le modÃ¨le pour gÃ©nÃ©rer la synthÃ¨se
        try:
            synthesis_response = await self.agent.arun(synthesis_prompt)
            return synthesis_response.content
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la synthÃ¨se : {e}")
            # Retourner une synthÃ¨se par dÃ©faut en cas d'erreur
            return " | ".join(text_results)

    def _generate_detailed_subtasks(self, user_request: str) -> List[Dict[str, Any]]:
        """
        GÃ©nÃ©rer des sous-tÃ¢ches dÃ©taillÃ©es pour une requÃªte utilisateur
        
        Args:
            user_request (str): La requÃªte originale de l'utilisateur
        
        Returns:
            List[Dict[str, Any]]: Liste des sous-tÃ¢ches dÃ©taillÃ©es
        """
        try:
            # PrÃ©parer le prompt pour la gÃ©nÃ©ration de sous-tÃ¢ches
            subtasks_prompt = """
            DÃ©compose la tÃ¢che suivante en sous-tÃ¢ches essentielles et non redondantes.
            
            TÃ¢che principale : {user_request}
            
            Instructions:
            1. Analyse la tÃ¢che en dÃ©tail
            2. Identifie les actions concrÃ¨tes nÃ©cessaires
            3. Ã‰vite les Ã©tapes redondantes de rapport de rÃ©sultat
            4. Concentre-toi sur les actions productives
            
            Format de rÃ©ponse REQUIS (JSON strict) :
            {{
                "subtasks": [
                    {{
                        "task_id": "identifiant_unique",
                        "description": "Description concise et prÃ©cise de la sous-tÃ¢che",
                        "priority": "haute|moyenne|basse"                    
                    }}
                ]
            }}
            """.format(user_request=user_request)
            
            # GÃ©nÃ©rer les sous-tÃ¢ches directement avec le client OpenAI
            response = self.client.chat.completions.create(
                model=self.llm_config.get('model', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": "Tu es un expert en dÃ©composition de tÃ¢ches complexes, privilÃ©giant la concision et l'efficacitÃ©."},
                    {"role": "user", "content": subtasks_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            
            # Extraire le contenu de la rÃ©ponse
            subtasks_response = response.choices[0].message.content
            
            # VÃ©rifier que la rÃ©ponse est une chaÃ®ne
            if not isinstance(subtasks_response, str):
                logger.error(f"La rÃ©ponse du modÃ¨le n'est pas une chaÃ®ne : {type(subtasks_response)}")
                raise ValueError("RÃ©ponse du modÃ¨le invalide")
            
            # Convertir la rÃ©ponse en liste de sous-tÃ¢ches
            try:
                subtasks_dict = json.loads(subtasks_response)
            except json.JSONDecodeError as json_err:
                logger.error(f"Erreur de dÃ©codage JSON : {json_err}")
                logger.error(f"RÃ©ponse reÃ§ue : {subtasks_response}")
                raise
            
            subtasks = subtasks_dict.get('subtasks', [])
            
            # VÃ©rifier que le format est correct
            if not isinstance(subtasks, list):
                raise ValueError("La rÃ©ponse n'est pas une liste de sous-tÃ¢ches")

            return subtasks
        
        except Exception as e:
            logger.error(f"Erreur lors de la gÃ©nÃ©ration des sous-tÃ¢ches : {e}")
            # Retourner une dÃ©composition par dÃ©faut si la gÃ©nÃ©ration Ã©choue
            return [
                {
                    "task_id": "task_1",
                    "description": f"Analyser et exÃ©cuter : {user_request}",
                    "priority": "haute"
                }
            ]

async def process_user_request(
    user_request: str, 
    debug_mode: bool = False,
    # Ajout des paramÃ¨tres de mÃ©moire et stockage
    db_url: str = 'postgresql+psycopg2://p4t:o3CCgX7StraZqvRH5GqrOFLuzt5R6C@vps-af24e24d.vps.ovh.net:30030/myboun',
    memory_table_name: str = "agent_memory",
    storage_table_name: str = "agent_sessions",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Traiter une requÃªte utilisateur de maniÃ¨re asynchrone avec l'orchestrateur
    
    Args:
        user_request (str): La requÃªte de l'utilisateur
        debug_mode (bool): Mode de dÃ©bogage
        db_url (str): URL de connexion Ã  la base de donnÃ©es PostgreSQL
        memory_table_name (str): Nom de la table de mÃ©moire
        storage_table_name (str): Nom de la table de stockage
        user_id (Optional[str]): Identifiant utilisateur
        session_id (Optional[str]): Identifiant de session
    
    Returns:
        Dict[str, Any]: RÃ©sultat du traitement de la requÃªte
    """
    try:
        orchestrator = OrchestratorAgent(
            debug_mode=debug_mode, 
            original_request=user_request,
            # Ajout des paramÃ¨tres de mÃ©moire et stockage
            db_url=db_url,
            memory_table_name=memory_table_name,
            storage_table_name=storage_table_name,
            user_id=user_id,
            session_id=session_id
        )
        result = await orchestrator.process_request(
            user_request=user_request,
            debug_mode=debug_mode
        )
        
        # Extraction de la synthÃ¨se
        synthesized_result = result.get('result', '')
        
        # DÃ©termination de l'agent utilisÃ©
        agent_used = 'Multi-Purpose Intelligence Team'
        task_results = result.get('task_results', {})
        
        # Cas spÃ©cial : rÃ©sultat unique
        if len(task_results) > 1:
            agent_used = list(task_results.values())[0].get('agent', agent_used)
        
        # Cas oÃ¹ aucun rÃ©sultat n'est disponible
        elif len(task_results) == 1:
            first_task = list(task_results.keys())[0]
            task_result = task_results[first_task]
            
            # Si c'est un RunResponse
            if hasattr(task_result, 'content'):
                synthesized_result = task_result.content
                agent_used = task_result.name if hasattr(task_result, 'name') else agent_used
            
            # Si c'est un dictionnaire
            elif isinstance(task_result, dict):
                # Essayer d'extraire le contenu de diffÃ©rentes maniÃ¨res
                if 'result' in task_result:
                    result_content = task_result['result']
                    if isinstance(result_content, dict):
                        synthesized_result = result_content.get('content', str(result_content))
                    elif isinstance(result_content, str):
                        synthesized_result = result_content
                
                agent_used = task_result.get('agent', agent_used)
        
        return {
            'query': user_request,
            'result': synthesized_result,
            'agent_used': agent_used,
            'metadata': {},
            'error': None
        }
    except Exception as e:
        logger.error(f"Erreur lors du traitement de la requÃªte : {e}")
        logger.error(traceback.format_exc())
        return {
            'query': user_request,
            'result': '',
            'agent_used': 'Multi-Purpose Intelligence Team',
            'metadata': {},
            'error': str(e)
        }

def get_orchestrator_agent(
    model_id: str = "gpt-4o-mini",
    enable_web_agent: bool = True,
    enable_api_knowledge_agent: bool = False,
    enable_data_analysis_agent: bool = False,
    enable_travel_planner: bool = False,
    db_url: str = 'postgresql+psycopg2://p4t:o3CCgX7StraZqvRH5GqrOFLuzt5R6C@vps-af24e24d.vps.ovh.net:30030/myboun',
    memory_table_name: str = "agent_memory",
    storage_table_name: str = "agent_sessions",
    # ParamÃ¨tres optionnels de l'agent
    name: str = "Orchestrator Agent",
    instructions: Optional[List[str]] = None,
    tools: Optional[List[Callable]] = None,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    **kwargs
) -> OrchestratorAgent:
    """
    CrÃ©er un agent orchestrateur avec configuration personnalisable
    
    Args:
        model_id (str): Identifiant du modÃ¨le OpenAI
        enable_web_agent (bool): Activer l'agent de recherche web
        enable_api_knowledge_agent (bool): Activer l'agent de connaissances API
        enable_data_analysis_agent (bool): Activer l'agent d'analyse de donnÃ©es
        enable_travel_planner (bool): Activer l'agent de planification de voyage
        db_url (str): URL de connexion Ã  la base de donnÃ©es PostgreSQL
        memory_table_name (str): Nom de la table de mÃ©moire
        storage_table_name (str): Nom de la table de stockage
    
    Returns:
        OrchestratorAgent: Agent orchestrateur configurÃ©
    """
    return OrchestratorAgent(
        model_id=model_id,
        enable_web_agent=enable_web_agent,
        enable_api_knowledge_agent=enable_api_knowledge_agent,
        enable_data_analysis_agent=enable_data_analysis_agent,
        enable_travel_planner=enable_travel_planner,
        # ParamÃ¨tres de mÃ©moire et stockage
        db_url=db_url,
        memory_table_name=memory_table_name,
        storage_table_name=storage_table_name,
        # ParamÃ¨tres optionnels de l'agent
        name=name,
        instructions=instructions,
        tools=tools,
        user_id=user_id,
        session_id=session_id,
        **kwargs
    )

# Exemple d'utilisation
if __name__ == "__main__":
    import logging
    import json

    # Configuration du logging
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    def extract_content(chunk):
        """
        Extraire le contenu textuel d'un chunk de diffÃ©rents types
        """
        # Si c'est un tuple
        if isinstance(chunk, tuple):
            # Si le tuple a plus d'un Ã©lÃ©ment, prendre le deuxiÃ¨me
            if len(chunk) > 1:
                chunk = chunk[1]
            else:
                chunk = ""
        
        # Si c'est une liste, convertir en chaÃ®ne
        if isinstance(chunk, list):
            chunk = " ".join(str(item) for item in chunk)
        
        # Convertir en chaÃ®ne si ce n'est pas dÃ©jÃ  une chaÃ®ne
        return str(chunk)

    def test_orchestrator():
        # CrÃ©er l'agent orchestrateur sans agent web
        orchestrator = get_orchestrator_agent(
            enable_web_agent=True  # Activer la recherche web
        )
        
        # Exemples de requÃªtes de test
        test_requests = [
            "Faire une analyse comparative des performances des startups tech en 2024"
        ]
        
        for request in test_requests:
            print(f"\nğŸš€ Traitement de la requÃªte : {request}")
            
            # Utiliser la gÃ©nÃ©ration de rÃ©ponse directe
            try:
                # RÃ©cupÃ©rer le gÃ©nÃ©rateur de rÃ©ponse
                resp = orchestrator.run(request)
                
                # Collecter et afficher les rÃ©sultats par morceaux
                print("\nğŸ“Š RÃ©sultats :")
                full_result = ""
                for chunk in resp:
                    # Extraire le contenu du chunk
                    chunk_content = extract_content(chunk)
                    
                    print(chunk_content, end='', flush=True)
                    full_result += chunk_content
                
                print("\n\nğŸ” RÃ©sumÃ© :")
                print(f"Longueur totale de la rÃ©ponse : {len(full_result)} caractÃ¨res")
            except Exception as e:
                print(f"Erreur lors de l'exÃ©cution : {e}")
                import traceback
                traceback.print_exc()
    
    # ExÃ©cuter le test
    test_orchestrator()