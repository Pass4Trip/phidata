from typing import Optional, Any, Dict, Callable
import os
import logging
from dotenv import load_dotenv
import json
import uuid
from datetime import datetime, timedelta
import requests
from requests import Response
from typing import Dict

from phi.agent import Agent, AgentMemory
from phi.model.openai import OpenAIChat
from phi.storage.agent.postgres import PgAgentStorage
from phi.memory.db.postgres import PgMemoryDb


# Charger les variables d'environnement
load_dotenv()

# Construction dynamique de l'URL de base de donn√©es PostgreSQL
def build_postgres_url():
    """
    Construire dynamiquement l'URL de connexion PostgreSQL √† partir des variables d'environnement
    
    Returns:
        str: URL de connexion PostgreSQL
    """
    db_host = os.getenv('DB_HOST', 'vps-af24e24d.vps.ovh.net')
    db_port = os.getenv('DB_PORT', '30030')
    db_name = os.getenv('DB_NAME', 'myboun')
    db_user = os.getenv('DB_USER', 'p4t')
    db_password = os.getenv('DB_PASSWORD', '')
    db_schema = os.getenv('DB_SCHEMA', 'ai')
    
    # Construire l'URL de connexion PostgreSQL avec le sch√©ma
    db_url = f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}?options=-c%20search_path%3D{db_schema}'
    
    return db_url

# G√©n√©rer l'URL de base de donn√©es
db_url = build_postgres_url()

model_id = os.getenv('model_id', 'gpt-4o-mini')

# Configuration du logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # R√©duire le niveau de log
handler = logging.StreamHandler()
handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

agent_storage_file: str = "orchestrator_agent_sessions.db"



def query_lightrag_execute(question: str, user_id: str = "vinh") -> Dict[str, Any]:
    """
    Effectue une requ√™te au endpoint LightRAG pour obtenir une r√©ponse.
    
    Args:
        question (str): La question √† poser
        user_id (str, optional): L'identifiant de l'utilisateur. Defaults to "vinh".
    
    Returns:
        Dict[str, Any]: La r√©ponse du endpoint LightRAG
    """
    url = "http://51.77.200.196:30080/query/"
    headers = {
        "accept": "application/json",
        "Content-Type": "application/json"
    }
    payload = {
        "question": question,
        "user_id": user_id,
        "vdb_filter": [user_id]
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()  # L√®ve une exception pour les codes d'erreur HTTP
        return response.json()
    except requests.RequestException as e:
        logger.error(f"Erreur lors de la requ√™te LightRAG : {e}")
        return {
            "status": "error",
            "content": f"Impossible de contacter le service LightRAG : {str(e)}"
        }



def get_agent_lightrag_read_memo(
    model_id: str = "gpt-4o-mini",
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    debug_mode: bool = False,
    stream: bool = False,
    conversation_history: Optional[list] = None,  # Param√®tre pour l'historique de session
    **kwargs
) -> Agent:

    # G√©n√©rer un session_id unique si non fourni
    if session_id is None:
        session_id = str(uuid.uuid4())
        logger.info(f"üÜî G√©n√©ration d'un nouvel identifiant de session : {session_id}")    

    # Pr√©parer les instructions initiales
    base_instructions = [
        "Ton nom est Agent LightRAG.",
        "Tu es un agent conversationnel sp√©cialis√© dans l'utilisation du service LightRAG.",
        "Tes objectifs sont :",
        "1. Pour CHAQUE requ√™te utilisateur :",
        "   - Utiliser OBLIGATOIREMENT l'outil query_lightrag",
        "   - Passer la requ√™te EXACTEMENT telle qu'elle est re√ßue",
        "2. Retourner la r√©ponse du endpoint LightRAG SANS MODIFICATION",
        "3. Si le endpoint retourne une erreur :",
        "   - Retourner l'erreur telle quelle",
        "   - Ne PAS essayer de g√©n√©rer une r√©ponse alternative",
        "4. Structure de la r√©ponse :",
        "   - Utiliser EXACTEMENT la structure retourn√©e par LightRAG",
        "   - NE PAS modifier le format ou le contenu",
        "5. Gestion des cas particuliers :",
        "   - En cas d'impossibilit√© d'utiliser LightRAG, retourner un message d'erreur",
        "   - Ne PAS chercher √† contourner ou remplacer le service",
        "6. Principes de base :",
        "   - Transparence totale sur l'utilisation de LightRAG",
        "   - Aucune modification ou interpr√©tation de la r√©ponse",
        "7. Contexte de l'outil :",
        "   - query_lightrag est l'UNIQUE moyen de r√©pondre aux requ√™tes"
    ]

    def query_lightrag(query: str):
        query_result = query_lightrag_execute(query, user_id)
        # Convertir le r√©sultat en une cha√Æne de texte lisible
        if isinstance(query_result, dict):
            # Si c'est un dictionnaire, convertir en cha√Æne JSON lisible
            return json.dumps(query_result, ensure_ascii=False)
        elif isinstance(query_result, str):
            return query_result
        else:
            return str(query_result)

    # Gestion de l'historique de session
    # --------------------------------
    # Exemple d'utilisation de l'historique de conversation
    # L'historique est pass√© depuis le gestionnaire de session WebSocket
    # Structure attendue : [{'role': 'user'/'assistant', 'content': 'message'}]
    if conversation_history:
        # Ajouter le contexte de la conversation pr√©c√©dente aux instructions
        context_instruction = "Contexte de la conversation pr√©c√©dente :"
        for msg in conversation_history:
            # Traduire le r√¥le pour plus de clart√©
            role = "Utilisateur" if msg['role'] == 'user' else "Assistant"
            context_instruction += f"\n- {role}: {msg['content']}"
        
        # Ins√©rer le contexte apr√®s la description initiale
        base_instructions.insert(3, context_instruction)

        # Commentaires sur les possibilit√©s d'utilisation de l'historique :
        # 1. Comprendre le contexte pr√©c√©dent
        # 2. √âviter les r√©p√©titions
        # 3. Maintenir la coh√©rence de la conversation
        # 4. Personnaliser les r√©ponses en fonction des interactions pr√©c√©dentes

    # Cr√©er l'agent Phidata
    agent_lightrag = Agent(
        instructions=base_instructions,
        model=OpenAIChat(
            model=model_id,
            temperature=0.7,
        ),
        #output_format="json",  # Sp√©cifier le format de sortie JSON
        debug_mode=debug_mode,  # Forcer le mode d√©bogage
        agent_id="agent_lightrag",
        user_id=user_id,
        session_id=session_id,
        name="Agent LightRAG",
        tools=[query_lightrag],  # Ajout des outils, dont LightRAG
        memory=AgentMemory(
            db=PgMemoryDb(table_name="web_searcher__memory", db_url=db_url),
            # Commentaires sur les options de m√©moire :
            # create_user_memories : Cr√©e des m√©moires personnalis√©es par utilisateur
            # update_user_memories_after_run : Met √† jour ces m√©moires apr√®s chaque ex√©cution
            # create_session_summary : Cr√©e un r√©sum√© de la session
            # update_session_summary_after_run : Met √† jour ce r√©sum√© apr√®s chaque ex√©cution
            create_user_memories=True,
            update_user_memories_after_run=True,
            create_session_summary=True,
            update_session_summary_after_run=True,
        ),        
        storage=PgAgentStorage(table_name="web_searcher_sessions", db_url=db_url),
    )

    logger.info("‚úÖ Agent LightRAG initialis√© avec succ√®s")
    return agent_lightrag



# Bloc main pour lancer l'agent directement
if __name__ == "__main__":
    import sys
    import argparse
    
    # Configuration de l'analyseur d'arguments
    parser = argparse.ArgumentParser(description="Lancer un agent de base en mode interactif")
    parser.add_argument("--model", default="gpt-4o-mini", help="Mod√®le OpenAI √† utiliser")
    parser.add_argument("--user_id", help="Identifiant utilisateur")
    parser.add_argument("--session_id", help="Identifiant de session")
    

    user_id = "vinh"

    # Analyser les arguments
    args = parser.parse_args()
    
    # Cr√©er l'agent
    agent = get_agent_lightrag_read_memo(
        model_id="gpt-4o-mini", 
        user_id=args.user_id, 
        session_id=args.session_id
    )
    
    # Mode interactif
    print("ü§ñ Agent LightRAG - Mode Interactif")
    print("Tapez 'exit' ou 'quit' pour quitter.")
    
    conversation_history = []
    
    while True:
        try:
            user_input = input("üë§ Votre demande : ")
            
            # Obtenir la r√©ponse de l'agent
            print(f"\nüîç Requ√™te : {user_input}")
            response = agent.run(user_input)

            # Ajouter l'historique de conversation
            conversation_history.append({'role': 'user', 'content': user_input})
            
            # G√©rer diff√©rents types de r√©ponses
            if hasattr(response, 'content'):
                content = response.content
            elif isinstance(response, str):
                content = response
            else:
                content = str(response)
            
            print(f"\nü§ñ Type de r√©ponse : {type(content)}")
            print(f"\nü§ñ Contenu de la r√©ponse : {content}")
            
            conversation_history.append({'role': 'assistant', 'content': content})

            # Afficher la r√©ponse
            print("\nü§ñ R√©ponse :", content)
        
        except KeyboardInterrupt:
            print("Au revoir ! üëã")
            break
        except Exception as e:
            print(f"\n‚ùå Erreur : {e}")
            break
